[
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Résumé",
    "section": "",
    "text": "Download Current Résumé"
  },
  {
    "objectID": "portfolio/posts/datafest (2024)/index.html",
    "href": "portfolio/posts/datafest (2024)/index.html",
    "title": "Mizzou Datafest 2024: Best-in-Show Submission",
    "section": "",
    "text": "The following analysis was the best-in-show submission for Mizzou’s 2024 Datafest competition.\nAmerican Statistical Association (ASA) DataFest is a celebration of data in which teams of undergraduates work around the clock to discover and share meaning in a large, rich, and complex data set provided by a corporate sponsor. It is a nationally coordinated weekend-long data analysis competition and challenges students to find their own story to tell with the data that is meaningful to the data donor. Teams are given only 22 hours to explore the data, identity meaningful insights, and create visualizations and reports to present to a panel of judges.\nCredit to everyone else on the Truman State DataFest team, .gif or .gif: Evan AuBuchon, Nathan Bresette, and Severin Hussey. It was a tremendous experience to work with all of them to produce the following analysis."
  },
  {
    "objectID": "portfolio/posts/datafest (2024)/index.html#project-information",
    "href": "portfolio/posts/datafest (2024)/index.html#project-information",
    "title": "Mizzou Datafest 2024: Best-in-Show Submission",
    "section": "",
    "text": "The following analysis was the best-in-show submission for Mizzou’s 2024 Datafest competition.\nAmerican Statistical Association (ASA) DataFest is a celebration of data in which teams of undergraduates work around the clock to discover and share meaning in a large, rich, and complex data set provided by a corporate sponsor. It is a nationally coordinated weekend-long data analysis competition and challenges students to find their own story to tell with the data that is meaningful to the data donor. Teams are given only 22 hours to explore the data, identity meaningful insights, and create visualizations and reports to present to a panel of judges.\nCredit to everyone else on the Truman State DataFest team, .gif or .gif: Evan AuBuchon, Nathan Bresette, and Severin Hussey. It was a tremendous experience to work with all of them to produce the following analysis."
  },
  {
    "objectID": "portfolio/posts/datafest (2024)/index.html#data",
    "href": "portfolio/posts/datafest (2024)/index.html#data",
    "title": "Mizzou Datafest 2024: Best-in-Show Submission",
    "section": "Data",
    "text": "Data\nAll data for this analysis was provided by CourseKata, an online Statistics & Data Science curriculum for college and high school students. Our clients, CourseKata’s curriculum writers, tasked our team with identifying and analyzing features that proved to have a profound effect on a student behavior and performance. To do this, the CourseKata team gave us access to 6 tables of data.\n\nThe page_views table contained information about student usage of the textbook, i.e. engagement statistics, idle time statistics, access dates, etc.\nThe responses table contained information about student response to questions within the online circular, i.e. question type, question answer, number of correct answers, etc.\nThe media_views table contained infomation about student usage of specific media formats in the online books. Particularly, the table gathers information about how students utilized the video based course material.\nThe items table contains information about individual questions and “items” within the different books, i.e. question formats, number of questions, etc.\nThe checkpoints_pulse table contained student attitudinal data. Specifically, at the beginning of every chapter students were tasked with taking “pulse survey” which would record their attitudes towards the previous chapter. This table is where we focused a majority of our initial analysis as the clients were invested in understanding student attitude towards the online material.\nThe checkpoints_eoc table contained student performance data. Specifically, how the student preformed on end of chapter assessments."
  },
  {
    "objectID": "portfolio/posts/datafest (2024)/index.html#exploratory-analysis",
    "href": "portfolio/posts/datafest (2024)/index.html#exploratory-analysis",
    "title": "Mizzou Datafest 2024: Best-in-Show Submission",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nWhat follows is a summary of the group’s key, preliminary, takeaways from an exploratory analysis of the data. For more concrete solutions and methodology, I suggest skipping this section and going straight to Cleaning and Engineering.\n\nPrefatory Data Cleaning & Engineering\nSimple, prefatory data engineering had to occur in order to preform exploratory analysis. For a full breakdown of our final data cleaning and engineering protocol, see the Cleaning & Engineering section.\n\n\nShow Code\neoc_agg &lt;- checkpoints_eoc %&gt;%\n  mutate(avg_attempt = n_attempt/n_possible) %&gt;% \n  filter(book == \"College / Statistics and Data Science (ABC)\", !is.na(EOC)) %&gt;% \n  select(-c(n_possible,n_correct,n_attempt,book))\n\npage_agg &lt;- page_views %&gt;%\n  filter(book == \"College / Statistics and Data Science (ABC)\") %&gt;%\n  mutate(idle = idle_brief + idle_long) %&gt;%\n  mutate(off_page = off_page_brief + off_page_long) %&gt;%\n  select(student_id, chapter_number, release, engaged, idle, off_page, tried_again_clicks, institution_id) %&gt;%\n  group_by(student_id, chapter_number, release) %&gt;%\n  summarise(engaged_sum = sum(engaged, na.rm = TRUE),\n            idle_sum = sum(idle, na.rm = TRUE) / 60000,\n            off_page_sum = sum(off_page, na.rm = TRUE) / 60000,\n            tried_again_clicks_sum = sum(tried_again_clicks, na.rm = TRUE))\n\ntotal_table &lt;- left_join(eoc_agg,page_agg, by = c(\"student_id\",\"chapter_number\"))\n\ntotal_table$off_page_sum &lt;- format(total_table$off_page_sum, scientific = FALSE)\n\n\n\n\nShow Code\npulse &lt;- checkpoints_pulse %&gt;% \n  mutate(cost = ifelse(construct == \"Cost\", response, NA)) %&gt;% \n  mutate(expectancy = ifelse(construct == \"Expectancy\", response, NA)) %&gt;% \n  mutate(intrinsic = ifelse(construct == \"Intrinsic Value\", response, NA)) %&gt;% \n  mutate(utility = ifelse(construct == \"Utility Value\", response, NA)) %&gt;%\n  filter(response != \"\")\n\npulse &lt;- pulse %&gt;% group_by(student_id, chapter_number) %&gt;%\n  summarize(cost = mean(cost, na.rm = TRUE), \n            expectancy = mean(expectancy, na.rm = TRUE), \n            intrinsic = mean(intrinsic, na.rm = TRUE), \n            utility = mean(utility, na.rm = TRUE)) %&gt;%\n  filter(cost != 3.5,expectancy != 3.5)\n\nfull_pulse &lt;- full_join(total_table, pulse, by = c(\"student_id\",\"chapter_number\"))\n\ncheckpoints_pulse &lt;- checkpoints_pulse %&gt;%  mutate(class_id = case_when(class_id == \"0089dedf-6316-4c32-a38c-d48dfafed882\" ~ \"C-01\",\n                              class_id == \"074123e7-cd90-4500-86fe-286aaa733bf5\" ~ \"C-02\",\n                              class_id == \"0d546479-6f77-4477-9c7e-365cd36c97eb\" ~ \"C-03\",\n                              class_id == \"1020418a-3eeb-4251-88f7-150c8fe00a56\" ~ \"C-04\",\n                              class_id == \"103f5ce8-9e95-4916-815e-9f821d274a59\" ~ \"C-05\",\n                              class_id == \"1cca9f91-5c4a-4e1a-8e0e-293b070dfd6f\" ~ \"C-06\",\n                              class_id == \"20bd524c-bb2d-4b74-a419-929475b91d94\" ~ \"C-07\",\n                              class_id == \"2294d558-6f5d-41c5-8d28-7b5280970f95\" ~ \"C-08\",\n                              class_id == \"3631cec9-51d3-4237-906f-a142a715be51\" ~ \"C-09\",\n                              class_id == \"40e49bfa-f6cb-42fa-a3a4-b23592b799ec\" ~ \"C-10\",\n                              class_id == \"4a3b5b2c-ef0f-4121-96f4-fd8a42764836\" ~ \"C-11\",\n                              class_id == \"51711479-441b-4c02-aef7-517aca63a53f\" ~ \"C-12\",\n                              class_id == \"52619962-72f6-4716-9c64-1c06fe10f739\" ~ \"C-13\",\n                              class_id == \"552ede8f-6b54-426d-8d29-abdc43a668cb\" ~ \"C-14\",\n                              class_id == \"5bd961c4-659c-40a7-a685-6735189f2b65\" ~ \"C-15\",\n                              class_id == \"60e05fa5-c986-4973-9833-16238720b727\" ~ \"C-16\",\n                              class_id == \"65246c1e-a176-4760-acb5-a320a9b7b2fe\" ~ \"C-17\",\n                              class_id == \"686478e7-82ac-4e6c-a3ec-2da0076ef868\" ~ \"C-18\",\n                              class_id == \"79662249-02f6-48d8-aa99-1e1c0aeea77d\" ~ \"C-19\",\n                              class_id == \"7a987176-7e55-45b5-a715-7f56c59d5f49\" ~ \"C-20\",\n                              class_id == \"822d72d9-0c18-47a0-99fc-7223b4fd22f5\" ~ \"C-21\",\n                              class_id == \"8589cd83-192c-44c8-b649-cd848e519530\" ~ \"C-22\",\n                              class_id == \"94da41a4-f9f8-4225-bf41-42db737850b9\" ~ \"C-23\",\n                              class_id == \"97c61e74-5a20-4cf5-bf67-8f8db750d0e7\" ~ \"C-24\",\n                              class_id == \"98119d92-8cc6-416a-972c-630351726223\" ~ \"C-25\",\n                              class_id == \"9bdf8bfc-9998-4fd8-85d2-70c91cf94891\" ~ \"C-26\",\n                              class_id == \"9fad0c9e-9d3d-4eed-ada6-3959bd6d712c\" ~ \"C-27\",\n                              class_id == \"afcb6b4e-a0c0-46ce-b38c-c96329c91471\" ~ \"C-28\",\n                              class_id == \"b1421b49-4026-4c61-9786-d4ef110c8db3\" ~ \"C-29\",\n                              class_id == \"b16b895d-ca1d-4330-a36d-c43fb33436e5\" ~ \"C-30\",\n                              class_id == \"bc650f4f-11f0-439a-a90a-47726724c811\" ~ \"C-31\",\n                              class_id == \"bcae937d-c95f-436c-ac0f-d4a5e995de19\" ~ \"C-32\",\n                              class_id == \"c09145c1-d635-41ae-b881-17ab46895fe4\" ~ \"C-33\",\n                              class_id == \"c1168ee3-7ac8-4fdc-af0e-e375ad0629fe\" ~ \"C-34\",\n                              class_id == \"c7008a64-b43c-4eb4-bebf-07b08b9894ad\" ~ \"C-35\",\n                              class_id == \"cc1ffb2e-5555-4109-8ad8-2d49cb54ad10\" ~ \"C-36\",\n                              class_id == \"d0b4f5e2-6d8f-4828-91cd-3f4714b821b0\" ~ \"C-37\",\n                              class_id == \"fe8c4185-7e8d-48eb-bf0e-85562e060d5d\" ~ \"C-38\"))\n\n\n\n\nKey EDA Findings\nOur initial goals were to…\n\nIdentify differences among student attitudes based on end of chapter surveys\nUnderstand what caused these difference in student attitudes\nIdentify if student attitudes affected student performance\n\n\n\nGraphs for Pulse, Attitudinal, Data\nAs noted above, our main interest was trying to variables that could explain and/or correlate with student attitudes (or vise versa). However, as the dozens of graphs and correlates bellow will illustrate, no such correlation exists. Even the few relationships that appear to exist, have such a low correlation value that they are not worth investigating.\n\n\nShow Code\nlibrary(wesanderson) # graph colors\nggplot(full_pulse, aes(x = cost, y = EOC)) +\n  theme_minimal() +\n  geom_violin(aes(color = as.factor(cost))) +\n  geom_boxplot(aes(fill = as.factor(cost)), width = 0.4, outliers = F) +\n  scale_fill_manual(values = wes_palette(name = \"Darjeeling1\",n=7, type = \"continuous\")) +\n  scale_color_manual(values = wes_palette(name = \"Darjeeling1\",n=7, type = \"continuous\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nShow Code\neoc_cost &lt;- lm(EOC ~ cost, full_pulse)\nsummary(eoc_cost)\n\n\n\nCall:\nlm(formula = EOC ~ cost, data = full_pulse)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.66104 -0.13379  0.02359  0.14846  0.45963 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.685177   0.005533  123.84   &lt;2e-16 ***\ncost        -0.024134   0.001730  -13.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1878 on 7918 degrees of freedom\n  (4870 observations deleted due to missingness)\nMultiple R-squared:  0.02398,   Adjusted R-squared:  0.02386 \nF-statistic: 194.5 on 1 and 7918 DF,  p-value: &lt; 2.2e-16\n\n\nShow Code\nggplot(full_pulse, aes(x = expectancy, y = EOC)) +\n  theme_minimal() +\n  geom_violin(aes(color = as.factor(expectancy))) +\n  geom_boxplot(aes(fill = as.factor(expectancy)), width = 0.4, outliers = F) +\n  scale_fill_manual(values = wes_palette(name = \"Darjeeling1\",n=7, type = \"continuous\")) +\n  scale_color_manual(values = wes_palette(name = \"Darjeeling1\",n=7, type = \"continuous\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nShow Code\neoc_exp &lt;- lm(EOC ~ expectancy, full_pulse)\nsummary(eoc_exp)\n\n\n\nCall:\nlm(formula = EOC ~ expectancy, data = full_pulse)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.69508 -0.12633  0.02348  0.14183  0.53882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.414394   0.008495   48.78   &lt;2e-16 ***\nexpectancy  0.046782   0.001933   24.20   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1835 on 7918 degrees of freedom\n  (4870 observations deleted due to missingness)\nMultiple R-squared:  0.06888,   Adjusted R-squared:  0.06876 \nF-statistic: 585.7 on 1 and 7918 DF,  p-value: &lt; 2.2e-16\n\n\nShow Code\nggplot(full_pulse, aes(x = intrinsic, y = EOC)) +\n  theme_minimal() +\n  geom_violin(aes(color = as.factor(intrinsic))) +\n  geom_boxplot(aes(fill = as.factor(intrinsic)), width = 0.4, outliers = F) +\n  scale_fill_manual(values = wes_palette(name = \"Darjeeling1\",n=7, type = \"continuous\")) +\n  scale_color_manual(values = wes_palette(name = \"Darjeeling1\",n=7, type = \"continuous\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nShow Code\neoc_int &lt;- lm(EOC ~ intrinsic, full_pulse)\nsummary(eoc_int)\n\n\n\nCall:\nlm(formula = EOC ~ intrinsic, data = full_pulse)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62761 -0.13339  0.02316  0.14605  0.49928 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.468999   0.009838   47.67   &lt;2e-16 ***\nintrinsic   0.031721   0.002122   14.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1874 on 7635 degrees of freedom\n  (5153 observations deleted due to missingness)\nMultiple R-squared:  0.02843,   Adjusted R-squared:  0.0283 \nF-statistic: 223.4 on 1 and 7635 DF,  p-value: &lt; 2.2e-16\n\n\nShow Code\nggplot(full_pulse, aes(x = utility, y = EOC)) +\n  theme_minimal() +\n  geom_violin(aes(color = as.factor(utility))) +\n  geom_boxplot(aes(fill = as.factor(utility)), width = 0.4, outliers = F) +\n  scale_fill_manual(values = wes_palette(name = \"Darjeeling1\",n=7, type = \"continuous\")) +\n  scale_color_manual(values = wes_palette(name = \"Darjeeling1\",n=7, type = \"continuous\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nShow Code\neoc_util &lt;- lm(EOC ~ utility, full_pulse)\nsummary(eoc_util)\n\n\n\nCall:\nlm(formula = EOC ~ utility, data = full_pulse)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67916 -0.13088  0.02291  0.14275  0.54392 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.411459   0.009913   41.51   &lt;2e-16 ***\nutility     0.044617   0.002135   20.90   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1849 on 7840 degrees of freedom\n  (4948 observations deleted due to missingness)\nMultiple R-squared:  0.05276,   Adjusted R-squared:  0.05264 \nF-statistic: 436.7 on 1 and 7840 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nShow Code\nggplot(full_pulse, aes(y = (avg_attempt)^-5, x = cost)) +\n  theme_minimal() +\n  geom_violin(aes(color = as.factor(cost))) +\n  geom_boxplot(aes(fill = as.factor(cost)), width = 0.4, outliers = F) +\n  scale_fill_manual(values = wes_palette(name = \"Darjeeling1\",n=7, type = \"continuous\")) +\n  scale_color_manual(values = wes_palette(name = \"Darjeeling1\",n=7, type = \"continuous\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nShow Code\natt_cost &lt;- lm((avg_attempt)^-5 ~ cost, full_pulse)\nsummary(att_cost)\n\n\n\nCall:\nlm(formula = (avg_attempt)^-5 ~ cost, data = full_pulse)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2672 -0.1500 -0.0297  0.1220  0.7331 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.2673155  0.0054064  49.445   &lt;2e-16 ***\ncost        -0.0001044  0.0016907  -0.062    0.951    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1835 on 7918 degrees of freedom\n  (4870 observations deleted due to missingness)\nMultiple R-squared:  4.819e-07, Adjusted R-squared:  -0.0001258 \nF-statistic: 0.003816 on 1 and 7918 DF,  p-value: 0.9507\n\n\nShow Code\nggplot(full_pulse, aes(y = (avg_attempt)^-5, x = expectancy)) +\n  theme_minimal() +\n  geom_violin(aes(color = as.factor(expectancy))) +\n  geom_boxplot(aes(fill = as.factor(expectancy)), width = 0.4, outliers = F) +\n  scale_fill_manual(values = wes_palette(name = \"Darjeeling1\",n=7, type = \"continuous\")) +\n  scale_color_manual(values = wes_palette(name = \"Darjeeling1\",n=7, type = \"continuous\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nShow Code\natt_exp &lt;- lm((avg_attempt)^-5 ~ expectancy, full_pulse)\nsummary(att_exp)\n\n\n\nCall:\nlm(formula = (avg_attempt)^-5 ~ expectancy, data = full_pulse)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27187 -0.14970 -0.02889  0.12106  0.73380 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.253912   0.008497  29.882   &lt;2e-16 ***\nexpectancy  0.003072   0.001933   1.589    0.112    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1835 on 7918 degrees of freedom\n  (4870 observations deleted due to missingness)\nMultiple R-squared:  0.0003186, Adjusted R-squared:  0.0001924 \nF-statistic: 2.524 on 1 and 7918 DF,  p-value: 0.1122\n\n\nShow Code\nggplot(full_pulse, aes(y = (avg_attempt)^-5, x = intrinsic)) +\n  theme_minimal() +\n  geom_violin(aes(color = as.factor(intrinsic))) +\n  geom_boxplot(aes(fill = as.factor(intrinsic)), width = 0.4, outliers = F) +\n  scale_fill_manual(values = wes_palette(name = \"Darjeeling1\",n=7, type = \"continuous\")) +\n  scale_color_manual(values = wes_palette(name = \"Darjeeling1\",n=7, type = \"continuous\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nShow Code\natt_int &lt;- lm((avg_attempt)^-5 ~ intrinsic, full_pulse)\nsummary(att_int)\n\n\n\nCall:\nlm(formula = (avg_attempt)^-5 ~ intrinsic, data = full_pulse)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2702 -0.1511 -0.0304  0.1220  0.7323 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.273789   0.009658   28.35   &lt;2e-16 ***\nintrinsic   -0.001520   0.002084   -0.73    0.466    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.184 on 7635 degrees of freedom\n  (5153 observations deleted due to missingness)\nMultiple R-squared:  6.971e-05, Adjusted R-squared:  -6.126e-05 \nF-statistic: 0.5323 on 1 and 7635 DF,  p-value: 0.4657\n\n\nShow Code\nggplot(full_pulse, aes(y = (avg_attempt)^-5, x = utility)) +\n  theme_minimal() +\n  geom_violin(aes(color = as.factor(utility))) +\n  geom_boxplot(aes(fill = as.factor(utility)), width = 0.4, outliers = F) +\n  scale_fill_manual(values = wes_palette(name = \"Darjeeling1\",n=7, type = \"continuous\")) +\n  scale_color_manual(values = wes_palette(name = \"Darjeeling1\",n=7, type = \"continuous\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nShow Code\natt_util &lt;- lm((avg_attempt)^-5 ~ utility, full_pulse)\nsummary(att_util)\n\n\n\nCall:\nlm(formula = (avg_attempt)^-5 ~ utility, data = full_pulse)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27257 -0.15009 -0.03074  0.12063  0.73487 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.249857   0.009844  25.381   &lt;2e-16 ***\nutility     0.003819   0.002120   1.801   0.0717 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1837 on 7840 degrees of freedom\n  (4948 observations deleted due to missingness)\nMultiple R-squared:  0.0004137, Adjusted R-squared:  0.0002862 \nF-statistic: 3.245 on 1 and 7840 DF,  p-value: 0.0717\n\n\n\n\nShow Code\nfull_pulse &lt;- full_pulse %&gt;%\n  filter(cost != 3.5)\nggplot(full_pulse, aes(y = log(engaged_sum), x = cost)) +\n  theme_minimal() +\n  geom_violin(aes(color = as.factor(cost))) +\n  geom_boxplot(aes(fill = as.factor(cost)), width = .4, outliers = F) +\n  scale_fill_manual(values = wes_palette(name = \"Darjeeling1\", n = 7, type = \"continuous\")) +\n  scale_color_manual(values = wes_palette(name = \"Darjeeling1\", n = 7, type = \"continuous\"))\n\n\n\n\n\n\n\n\n\nShow Code\neng_cost &lt;- lm(engaged_sum ~ cost, full_pulse)\nsummary(eng_cost)\n\n\n\nCall:\nlm(formula = engaged_sum ~ cost, data = full_pulse)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-10190276  -5244395  -1959925   2634441 134399410 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10437973     255886  40.792  &lt; 2e-16 ***\ncost         -247698      80024  -3.095  0.00197 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8687000 on 7918 degrees of freedom\n  (3057 observations deleted due to missingness)\nMultiple R-squared:  0.001209,  Adjusted R-squared:  0.001082 \nF-statistic: 9.581 on 1 and 7918 DF,  p-value: 0.001973\n\n\nShow Code\nggplot(full_pulse, aes(y = log(engaged_sum), x = expectancy)) +\n  theme_minimal() +\n  geom_violin(aes(color = as.factor(expectancy))) +\n  geom_boxplot(aes(fill = as.factor(expectancy)), width = .4, outliers = F) +\n  scale_fill_manual(values = wes_palette(name = \"Darjeeling1\", n = 7, type = \"continuous\")) +\n  scale_color_manual(values = wes_palette(name = \"Darjeeling1\", n = 7, type = \"continuous\"))\n\n\n\n\n\n\n\n\n\nShow Code\neng_exp &lt;- lm(engaged_sum ~ expectancy, full_pulse)\nsummary(eng_exp)\n\n\n\nCall:\nlm(formula = engaged_sum ~ expectancy, data = full_pulse)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n -9851194  -5243308  -1958462   2594304 134410511 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  9348951     402460  23.230   &lt;2e-16 ***\nexpectancy     83707      91577   0.914    0.361    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8692000 on 7918 degrees of freedom\n  (3057 observations deleted due to missingness)\nMultiple R-squared:  0.0001055, Adjusted R-squared:  -2.077e-05 \nF-statistic: 0.8355 on 1 and 7918 DF,  p-value: 0.3607\n\n\nShow Code\nggplot(full_pulse, aes(y = log(engaged_sum), x = intrinsic)) +\n  theme_minimal() +\n  geom_violin(aes(color = as.factor(intrinsic))) +\n  geom_boxplot(aes(fill = as.factor(intrinsic)), width = .4, outliers = F) +\n  scale_fill_manual(values = wes_palette(name = \"Darjeeling1\", n = 7, type = \"continuous\")) +\n  scale_color_manual(values = wes_palette(name = \"Darjeeling1\", n = 7, type = \"continuous\"))\n\n\n\n\n\n\n\n\n\nShow Code\neng_int &lt;- lm(engaged_sum ~ intrinsic, full_pulse)\nsummary(eng_int)\n\n\n\nCall:\nlm(formula = engaged_sum ~ intrinsic, data = full_pulse)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-10778838  -5281410  -1944226   2617144 134781044 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6382067     456669  13.975  &lt; 2e-16 ***\nintrinsic     732795      98516   7.438 1.13e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8699000 on 7635 degrees of freedom\n  (3340 observations deleted due to missingness)\nMultiple R-squared:  0.007195,  Adjusted R-squared:  0.007065 \nF-statistic: 55.33 on 1 and 7635 DF,  p-value: 1.13e-13\n\n\nShow Code\nggplot(full_pulse, aes(y = log(engaged_sum), x = utility)) +\n  theme_minimal() +\n  geom_violin(aes(color = as.factor(utility))) +\n  geom_boxplot(aes(fill = as.factor(utility)), width = .4, outliers = F) +\n  scale_fill_manual(values = wes_palette(name = \"Darjeeling1\", n = 7, type = \"continuous\")) +\n  scale_color_manual(values = wes_palette(name = \"Darjeeling1\", n = 7, type = \"continuous\"))\n\n\n\n\n\n\n\n\n\nShow Code\neng_util &lt;- lm(engaged_sum ~ utility, full_pulse)\nsummary(eng_util)\n\n\n\nCall:\nlm(formula = engaged_sum ~ utility, data = full_pulse)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-10884010  -5237550  -1926090   2630936 134800510 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6113322     465319  13.138  &lt; 2e-16 ***\nutility       795115     100223   7.933 2.43e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8681000 on 7840 degrees of freedom\n  (3135 observations deleted due to missingness)\nMultiple R-squared:  0.007964,  Adjusted R-squared:  0.007838 \nF-statistic: 62.94 on 1 and 7840 DF,  p-value: 2.426e-15\n\n\n\n\nLast Ditch Effort\nAs one more try to find meaning insight within the pulse question answers, one of our team mates even preformed some feature engineering to create new variables with the student attitudinal data (one example can be seen in the following code chunk). Similarly, it did not reveal any meaningful insight.\n\n\nShow Code\n#Boxplot of created combo variable with a fill of construct\ncheck &lt;- checkpoints_pulse %&gt;% mutate(combo = paste(construct, chapter_number, sep = \" \"))\n\nggplot(check, aes(x=combo,y=response, fill = construct)) + \n  geom_boxplot(outliers = F) +\n  theme_minimal() +\n  labs(title = \"Boxplot of Pulse Responses by Construct and Chapter\") +\n  scale_fill_manual(values = wes_palette(name=\"Darjeeling1\",n=4,type = \"discrete\")) +\n  theme(legend.position = \"none\",axis.text.x=element_text(angle=45),plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nggplot(checkpoints_pulse, aes(y = response, fill = construct)) +\n  theme_minimal() +\n  geom_boxplot(outliers = F) +\n  facet_wrap(~class_id)  +\n  scale_fill_manual(values = wes_palette(name = \"Darjeeling1\", n = 4, type = \"discrete\"))\n\n\n\n\n\n\n\n\n\n\n\nNew Exploratory Goals\nSince investigating subjective, student attitudinal responses lead to no significant insights, we decided to shift our analysis to focus on student performance. Thus, our new exploratory goals were…\n\nfind identifiers for student success.\ndetermine what an “excelling” student is and what a “struggling” student is.\nidentity the differences between excelling and struggling students.\n\n\n\nDifference Between Books\nSince the data frame contained three books, we wanted to check if the book affected student EOC scores.\n\n\nShow Code\n#Boxplot and violin plot of books by EOC\nggplot(checkpoints_eoc, aes(x = book, y = EOC)) +\n  geom_violin(fill = \"lightblue\", color = \"blue\", alpha = 0.5) +\n  geom_boxplot() +\n  labs(x = \"Books\",\n       title = \"Books by EOC\") +\n  scale_x_discrete(labels = c(\"College (ABC)\", \"Advanced College (ABCD)\", \"High School\")) +\n  theme(plot.title = element_text(hjust = 0.5)) \n\n\n\n\n\n\n\n\n\nFrom the plot it appears that the book does causes a difference in median EOC score, especially when comparing the College (ABC) book to the High School book. Since there was a difference in score, and the books contained chapter numbers and material, we decided to only analysis the EOC scores of students who utilized the College (ABC) book. This was our decision due to the College (ABC) book having the more students using it than the other two books, meaning we were retain a larger sample size.\nThe rest of this analysis is thus only for the College (ABC) book and its various versions.\n\n\nFurther Exploration of EOC Scores/Student Performance\n\n\nShow Code\neoc &lt;- checkpoints_eoc %&gt;% filter(book == \"College / Statistics and Data Science (ABC)\")\neoc &lt;- eoc %&gt;% filter(EOC != \"\")\neoc &lt;- eoc %&gt;% mutate(avg_try = n_attempt/n_possible)\n\neoc &lt;- eoc %&gt;%  mutate(class_id = case_when(class_id == \"0089dedf-6316-4c32-a38c-d48dfafed882\" ~ \"C-01\",\n                              class_id == \"074123e7-cd90-4500-86fe-286aaa733bf5\" ~ \"C-02\",\n                              class_id == \"0d546479-6f77-4477-9c7e-365cd36c97eb\" ~ \"C-03\",\n                              class_id == \"1020418a-3eeb-4251-88f7-150c8fe00a56\" ~ \"C-04\",\n                              class_id == \"103f5ce8-9e95-4916-815e-9f821d274a59\" ~ \"C-05\",\n                              class_id == \"1cca9f91-5c4a-4e1a-8e0e-293b070dfd6f\" ~ \"C-06\",\n                              class_id == \"20bd524c-bb2d-4b74-a419-929475b91d94\" ~ \"C-07\",\n                              class_id == \"2294d558-6f5d-41c5-8d28-7b5280970f95\" ~ \"C-08\",\n                              class_id == \"3631cec9-51d3-4237-906f-a142a715be51\" ~ \"C-09\",\n                              class_id == \"40e49bfa-f6cb-42fa-a3a4-b23592b799ec\" ~ \"C-10\",\n                              class_id == \"4a3b5b2c-ef0f-4121-96f4-fd8a42764836\" ~ \"C-11\",\n                              class_id == \"51711479-441b-4c02-aef7-517aca63a53f\" ~ \"C-12\",\n                              class_id == \"52619962-72f6-4716-9c64-1c06fe10f739\" ~ \"C-13\",\n                              class_id == \"552ede8f-6b54-426d-8d29-abdc43a668cb\" ~ \"C-14\",\n                              class_id == \"5bd961c4-659c-40a7-a685-6735189f2b65\" ~ \"C-15\",\n                              class_id == \"60e05fa5-c986-4973-9833-16238720b727\" ~ \"C-16\",\n                              class_id == \"65246c1e-a176-4760-acb5-a320a9b7b2fe\" ~ \"C-17\",\n                              class_id == \"686478e7-82ac-4e6c-a3ec-2da0076ef868\" ~ \"C-18\",\n                              class_id == \"79662249-02f6-48d8-aa99-1e1c0aeea77d\" ~ \"C-19\",\n                              class_id == \"7a987176-7e55-45b5-a715-7f56c59d5f49\" ~ \"C-20\",\n                              class_id == \"822d72d9-0c18-47a0-99fc-7223b4fd22f5\" ~ \"C-21\",\n                              class_id == \"8589cd83-192c-44c8-b649-cd848e519530\" ~ \"C-22\",\n                              class_id == \"94da41a4-f9f8-4225-bf41-42db737850b9\" ~ \"C-23\",\n                              class_id == \"97c61e74-5a20-4cf5-bf67-8f8db750d0e7\" ~ \"C-24\",\n                              class_id == \"98119d92-8cc6-416a-972c-630351726223\" ~ \"C-25\",\n                              class_id == \"9bdf8bfc-9998-4fd8-85d2-70c91cf94891\" ~ \"C-26\",\n                              class_id == \"9fad0c9e-9d3d-4eed-ada6-3959bd6d712c\" ~ \"C-27\",\n                              class_id == \"afcb6b4e-a0c0-46ce-b38c-c96329c91471\" ~ \"C-28\",\n                              class_id == \"b1421b49-4026-4c61-9786-d4ef110c8db3\" ~ \"C-29\",\n                              class_id == \"b16b895d-ca1d-4330-a36d-c43fb33436e5\" ~ \"C-30\",\n                              class_id == \"bc650f4f-11f0-439a-a90a-47726724c811\" ~ \"C-31\",\n                              class_id == \"bcae937d-c95f-436c-ac0f-d4a5e995de19\" ~ \"C-32\",\n                              class_id == \"c09145c1-d635-41ae-b881-17ab46895fe4\" ~ \"C-33\",\n                              class_id == \"c1168ee3-7ac8-4fdc-af0e-e375ad0629fe\" ~ \"C-34\",\n                              class_id == \"c7008a64-b43c-4eb4-bebf-07b08b9894ad\" ~ \"C-35\",\n                              class_id == \"cc1ffb2e-5555-4109-8ad8-2d49cb54ad10\" ~ \"C-36\",\n                              class_id == \"d0b4f5e2-6d8f-4828-91cd-3f4714b821b0\" ~ \"C-37\",\n                              class_id == \"fe8c4185-7e8d-48eb-bf0e-85562e060d5d\" ~ \"C-38\"))\n\neoc_temp &lt;- eoc\neoc_temp$chapter_number &lt;-  as.factor(eoc_temp$chapter_number)\n\n#EOC with fill by chapter_number\nggplot(eoc_temp, aes(y = EOC, fill = chapter_number)) +\n  theme_minimal() +\n  geom_boxplot(outliers = F) +\n  facet_wrap(~class_id)  +\n  scale_fill_manual(values = wes_palette(name = \"Darjeeling1\", n = 13, type = \"continuous\"))\n\n\n\n\n\n\n\n\n\n\n\nShow Code\n#Boxplot of chapters by EOC\nggplot(data = checkpoints_eoc, aes(x = as.factor(chapter_number), y = EOC)) +\n  geom_boxplot(outliers = FALSE, aes(color = as.factor(chapter_number)), show.legend = FALSE) +  \n  labs(title = \"Boxplot of Chapters by EOC\",\n       x = \"Chapter Number\") +\n  scale_color_manual(values = wes_palette(\"Darjeeling1\", n = 16, type = \"continuous\")) +  \n  theme_minimal() +\n  geom_hline(yintercept = 0.6, linetype = \"dashed\", color = \"black\", size = 1.5) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nFrom the visual analysis above, it does appear that EOC has a relationship with both Chapter Number and Class-ID, as the EOC does change significantly as the Chapter increases and the classes change.\n\n\nCreating Response Variable\nSince we were able to find some correlations for the EOC variable, and the hackathon was a limited time event, we decided to pursue student performance as our main research/response variable. In order to simplify the analysis, we invesitaged with converting the continuous EOC variable into a binary variable for if the student was passing/failing the class.\n\n\nShow Code\n#Creating success variable by .6 &gt; is pass and below is fail\ncheckpoints_eoc$success &lt;- ifelse(checkpoints_eoc$EOC &gt; .6, \"P\", \"F\")\n\n#Histogram of EOC\n#install.packages(\"wesanderson\")\nlibrary(wesanderson)\n\n#colors for plot\ndesired_color &lt;- wes_palette(\"Darjeeling1\")[1]  \ndesired_color2 &lt;- wes_palette(\"Darjeeling1\")[5]  \n\nggplot(data = checkpoints_eoc, aes(x = EOC, fill = success)) +\n  geom_histogram(color = \"black\", binwidth = 0.1, breaks = c(0, 0.05, 0.1, .15, 0.2, .25, 0.3, .35, 0.4, .45, 0.5, .55, 0.6, .65, 0.7, .75, 0.8, .85, 0.9, .95, 1)) +\n  labs(x = \"EOC\",\n       y = \"Count\",\n       title = \"Histogram of EOC by Pass/Fail\",\n       fill = \"Pass/Fail\") +\n  scale_fill_manual(values = c(desired_color, desired_color2)) +  \n  theme_minimal() +\n  theme(legend.position = \"top\", plot.title = element_text(hjust = 0.5)) \n\n\n\n\n\n\n\n\n\nWe used a student grade of 0.6 (a D in the standard American letter grading system) as our cut off for a stugulling/failing student and an excelling one. Making the binary variable on this cusp turned out to be advantageous, as when our categories (F/P) are roughly 50/50 split - which is ideal for any categorical based model or algorithm.\n\n\nNext Analysis Steps\nAfter determining a direction and response variable for our analysis, we created a single cleaning script to produce a clean data frame for any future modeling and analysis. With this clean data, we aimed to create an exploratory/predictive model that would identify the most important variables in if a student is failing or excelling in their CourseKata material.\nFor more information on our next steps, see the Cleaning & Engineering, Modeling, and Final Findings sections."
  },
  {
    "objectID": "portfolio/posts/datafest (2024)/index.html#cleaning-and-engineering",
    "href": "portfolio/posts/datafest (2024)/index.html#cleaning-and-engineering",
    "title": "Mizzou Datafest 2024: Best-in-Show Submission",
    "section": "Cleaning and Engineering",
    "text": "Cleaning and Engineering\n\nUsed Libraries\nTo feature engineer and clean the given data, we utilized dplyr and other tidyverse packages. Dplyr gave us a large range of data manipulation tools such as table joining, filtering, mutating, selecting, etc.\n\n\nShow Code\nlibrary(tidyverse)\n\n\n\n\nUsed Data\nBased on the results of our exploratory analysis, we focused our attention on trying to find indicators for student performance within non-attitudinal based data. We were unable to identify any correlation between student attitude and performance in our exploratory data analysis. Subsequently, we did not include any data from the pulse checkpoints table.\nThis left us with only two primary tables to load, join, engineer, and clean.\n\nDifference Between Books\nThe provided data contained information from three CourseKata text books: College, Advanced College, and High School.\n\n\nShow Code\n#Boxplot and violin plot of books by EOC\nggplot(checkpoints_eoc, aes(x = book, y = EOC)) +\n  geom_violin(fill = \"lightblue\", color = \"blue\", alpha = 0.5) +\n  geom_boxplot() +\n  labs(x = \"Books\",\n       title = \"Books by EOC\") +\n  scale_x_discrete(labels = c(\"College (ABC)\", \"Advanced College (ABCD)\", \"High School\")) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) \n\n\n\n\n\n\n\n\n\nThe plot shows that student performance (end of chapter score) differs from book to book - especially when comparing the College (ABC) book to the High School book. Additionally, the books share chapter numbers and chapter names, but contain different content for each chapter. Since there is a difference in the EOC scores, and the books cover different material, we decided to only perform further analysis on one of the three books. We choose the College (ABC) book because it had the largest number of recorded students, meaning that we were retain a larger sample size using that book over the other two.\nThe rest of this analysis is thus only for the College (ABC) book and its various versions and editions.\n\n\n\nCHECK_EOC Table\nThe checkpoints_eoc table contains information about each students’ performance on end of chapter exams. The EOC variable, which serves as a measure for student performance, is the ratio of the number of correctly answered questions to the number of potentially asked questions.\n\n\nShow Code\nCHECK_EOC &lt;- read.csv(\"data/checkpoints_eoc.csv\") #loading data\n\nfiltered_eoc &lt;- CHECK_EOC %&gt;%\n  mutate(avg_attempt = n_attempt/n_possible) %&gt;% #creating metric to see how many attempts per possible question there are\n  filter(book == \"College / Statistics and Data Science (ABC)\", !is.na(EOC)) %&gt;% #filtering by book with largest amount of data\n  select(-c(n_possible,n_correct,n_attempt,book)) \n\n\nThings to note in the code above:\n\nWe created a new variable avg_attempt which is the ratio of attempted questions and total possible questions.\nWe filtered the data set to only contain student information from the College (ABC).\nWe removed the variables n_possible, n_correct, and n_attempt and book. The n_… variables were removed because they were utilized during feature engineering and were directly correlated to student EOC score, thus being colinear and vestigial while modeling.\n\n\n\nPAGE_VIEW Table\nThe page_views table contains information about students’ use of individual text book pages such as time spent engaged on page, or time spent idle (not interacting with the textbook page). We aggregated many of these “time spent variables” to chapter sums of “time spent doing…” for each student.\n\n\nShow Code\n#|output: false\n\n\nPAGE_VIEW &lt;- read.csv(\"data/page_views.csv\") #loading data\n\nfiltered_views &lt;- PAGE_VIEW %&gt;% \n  filter(book == \"College / Statistics and Data Science (ABC)\") %&gt;%\n  mutate(idle = idle_brief + idle_long) %&gt;% #combining both similar time columns\n  mutate(off_page = off_page_brief + off_page_long) %&gt;% #combining both similar time columns\n  select(student_id, chapter_number, institution_id, release, engaged, idle, off_page, tried_again_clicks) %&gt;%\n  group_by(student_id, institution_id, chapter_number, release) %&gt;%\n  summarise(engaged_sum = sum(engaged, na.rm = T) / 60000, #convert to minutes from milliseconds\n            idle_sum = sum(idle, na.rm = T) / 60000, #convert to minutes from milliseconds\n            off_page_sum = as.numeric(format(sum(off_page, na.rm = T) / 60000, scientific = F)), #convert to minutes from milliseconds, also removed an issue with the variable being coded in scientific notation\n            tried_again_clicks_sum = sum(tried_again_clicks, na.rm = T))\n\n\n`summarise()` has grouped output by 'student_id', 'institution_id',\n'chapter_number'. You can override using the `.groups` argument.\n\n\nThings to note in the code above:\n\nSimilarly to the CHECK_EOC table, we filtered observations for only the ABC college textbook.\nWe combined the idle variables into one singular, summed variable.\nWe combined the off_page variable into a singular, summed variable.\nThe table was grouped by student_id, institution_id, chapter_number, and the release version of the book. Essentially, each observation of the table is a student-chapter pair (each student appears in the table 12 times because there are 12 chapters in the book) and various metrics based on the student’s performance for that individual chapter.\nSince we grouped the table, we also aggregated the idle, off_page, and tried_again_clicks to be sums. It is important to note that any time length/interval variable was converted from milliseconds to minutes.\n\n\n\nJoining Tables\n\n\nShow Code\nDATA &lt;- left_join(filtered_eoc,filtered_views, by = c(\"student_id\",\"chapter_number\")) # final table\n\n\nBoth tables were joined together using student_id and chapter_number as the unique identifiers.\n\n\nData Cleaning on Final Table\n\nMaking Understandable Institution and Class Names\nThe given names for the institutions classes were uninterruptible, thus we assigned our own values to be the unique IDs of the institutions and classes. We utilized a similar naming convention for both variables: I/C-#.\n\n\nShow Code\n#renaming institutions and classes\nDATA &lt;- DATA %&gt;%\n  mutate(institution_id = case_when(institution_id == \"04157183-8665-400a-925d-3bbb70ffe45e\" ~ \"I-01\",\n                                    institution_id == \"292cff87-3c74-4e94-8622-233afb0427dd\" ~ \"I-02\",\n                                    institution_id == \"364da48a-e0b2-4507-bc31-e7761fe16e95\" ~ \"I-03\",\n                                    institution_id == \"94a809a9-a0ef-4c47-8d96-3a5ad76f674b\" ~ \"I-04\",\n                                    institution_id == \"97aebe75-a051-4bff-a2c0-1d53eb5d9498\" ~ \"I-05\",\n                                    institution_id == \"d2e6c885-36f4-48b9-988b-42eef1f8ed9d\" ~ \"I-06\",\n                                    institution_id == \"f17495c5-e105-492d-878a-07a03ea3f805\" ~ \"I-07\",\n                                    institution_id == \"fc5f1b1b-2aeb-4e09-93fc-06fdac0d8030\" ~ \"I-08\")) %&gt;%\n  mutate(class_id = case_when(class_id == \"0089dedf-6316-4c32-a38c-d48dfafed882\" ~ \"C-01\",\n                              class_id == \"074123e7-cd90-4500-86fe-286aaa733bf5\" ~ \"C-02\",\n                              class_id == \"0d546479-6f77-4477-9c7e-365cd36c97eb\" ~ \"C-03\",\n                              class_id == \"1020418a-3eeb-4251-88f7-150c8fe00a56\" ~ \"C-04\",\n                              class_id == \"103f5ce8-9e95-4916-815e-9f821d274a59\" ~ \"C-05\",\n                              class_id == \"1cca9f91-5c4a-4e1a-8e0e-293b070dfd6f\" ~ \"C-06\",\n                              class_id == \"20bd524c-bb2d-4b74-a419-929475b91d94\" ~ \"C-07\",\n                              class_id == \"2294d558-6f5d-41c5-8d28-7b5280970f95\" ~ \"C-08\",\n                              class_id == \"3631cec9-51d3-4237-906f-a142a715be51\" ~ \"C-09\",\n                              class_id == \"40e49bfa-f6cb-42fa-a3a4-b23592b799ec\" ~ \"C-10\",\n                              class_id == \"4a3b5b2c-ef0f-4121-96f4-fd8a42764836\" ~ \"C-11\",\n                              class_id == \"51711479-441b-4c02-aef7-517aca63a53f\" ~ \"C-12\",\n                              class_id == \"52619962-72f6-4716-9c64-1c06fe10f739\" ~ \"C-13\",\n                              class_id == \"552ede8f-6b54-426d-8d29-abdc43a668cb\" ~ \"C-14\",\n                              class_id == \"5bd961c4-659c-40a7-a685-6735189f2b65\" ~ \"C-15\",\n                              class_id == \"60e05fa5-c986-4973-9833-16238720b727\" ~ \"C-16\",\n                              class_id == \"65246c1e-a176-4760-acb5-a320a9b7b2fe\" ~ \"C-17\",\n                              class_id == \"686478e7-82ac-4e6c-a3ec-2da0076ef868\" ~ \"C-18\",\n                              class_id == \"79662249-02f6-48d8-aa99-1e1c0aeea77d\" ~ \"C-19\",\n                              class_id == \"7a987176-7e55-45b5-a715-7f56c59d5f49\" ~ \"C-20\",\n                              class_id == \"822d72d9-0c18-47a0-99fc-7223b4fd22f5\" ~ \"C-21\",\n                              class_id == \"8589cd83-192c-44c8-b649-cd848e519530\" ~ \"C-22\",\n                              class_id == \"94da41a4-f9f8-4225-bf41-42db737850b9\" ~ \"C-23\",\n                              class_id == \"97c61e74-5a20-4cf5-bf67-8f8db750d0e7\" ~ \"C-24\",\n                              class_id == \"98119d92-8cc6-416a-972c-630351726223\" ~ \"C-25\",\n                              class_id == \"9bdf8bfc-9998-4fd8-85d2-70c91cf94891\" ~ \"C-26\",\n                              class_id == \"9fad0c9e-9d3d-4eed-ada6-3959bd6d712c\" ~ \"C-27\",\n                              class_id == \"afcb6b4e-a0c0-46ce-b38c-c96329c91471\" ~ \"C-28\",\n                              class_id == \"b1421b49-4026-4c61-9786-d4ef110c8db3\" ~ \"C-29\",\n                              class_id == \"b16b895d-ca1d-4330-a36d-c43fb33436e5\" ~ \"C-30\",\n                              class_id == \"bc650f4f-11f0-439a-a90a-47726724c811\" ~ \"C-31\",\n                              class_id == \"bcae937d-c95f-436c-ac0f-d4a5e995de19\" ~ \"C-32\",\n                              class_id == \"c09145c1-d635-41ae-b881-17ab46895fe4\" ~ \"C-33\",\n                              class_id == \"c1168ee3-7ac8-4fdc-af0e-e375ad0629fe\" ~ \"C-34\",\n                              class_id == \"c7008a64-b43c-4eb4-bebf-07b08b9894ad\" ~ \"C-35\",\n                              class_id == \"cc1ffb2e-5555-4109-8ad8-2d49cb54ad10\" ~ \"C-36\",\n                              class_id == \"d0b4f5e2-6d8f-4828-91cd-3f4714b821b0\" ~ \"C-37\",\n                              class_id == \"fe8c4185-7e8d-48eb-bf0e-85562e060d5d\" ~ \"C-38\"))\n\n\n\n\nMaking Categorical EOC Variable\nWe created a new variable called grade from the continuous EOC variable based on the standard American letter grading system. This variable ended up not be used in later analysis.\n\n\nShow Code\n# making cat. grade var.\nDATA$grade &lt;- ifelse(DATA$EOC &gt; .90, \"A\",\n                   ifelse(DATA$EOC &gt;= .80, \"B\",\n                          ifelse(DATA$EOC &gt;= .70, \"C\",\n                                 ifelse(DATA$EOC &gt;= .60, \"D\", \"F\")\n                                        )))\n\n\n\n\nMaking Binary Categorical EOC Variable\nThe main variable for our analysis was a binary categorical EOC variable called success. This variable indicated if a student was being “successful” or not in the class. We utilized a cut of of 0.6/60% as our cutoff because (1) it indicates a failing grade in the standard American letter grading system and (2) it proportioned our responses into roughly equal categories which was important for modeling and further analysis.\n\n\nShow Code\n# making bin. grade var for model\nDATA$success &lt;- ifelse(DATA$EOC &gt; .6, \"P\", \"F\")\n\n\n\nGraph of Binary Categorical Response Variable\n\n\nShow Code\n#colors for graph\nlibrary(wesanderson)\ndesired_color &lt;- wes_palette(\"Darjeeling1\")[1]  \ndesired_color2 &lt;- wes_palette(\"Darjeeling1\")[5]  \n\nDATA %&gt;%\n  ggplot(aes(x = EOC, fill = success)) +\n  geom_histogram(color = \"black\", binwidth = 0.1, breaks = c(0, 0.05, 0.1, .15, 0.2, .25, 0.3, .35, 0.4, .45, 0.5, .55, 0.6, .65, 0.7, .75, 0.8, .85, 0.9, .95, 1)) +\n  labs(x = \"EOC\",\n       y = \"Count\",\n       title = \"Histogram of EOC by Pass/Fail\",\n       fill = \"Pass/Fail\") +\n  scale_fill_manual(values = c(desired_color, desired_color2)) +  \n  theme_minimal() +\n  theme(legend.position = \"top\", plot.title = element_text(hjust = 0.5)) \n\n\n\n\n\n\n\n\n\n\n\n\n\nChanging Variable Types\nThis code chuck ensures that all of our variables are of the correct type within our R environment.\n\n\nShow Code\nDATA &lt;- DATA %&gt;% \n  mutate_if(is.character, as.factor) %&gt;%\n  mutate(chapter_number = as.factor(chapter_number))\n\n\n\n\nSaving Data\nLastly, we saved our data as a .Rdata file because it preserves all characteristics of an R data frame (like variable types and categorical levels/labels) - unlike a .csv or .txt file.\n\n\nShow Code\nsave(DATA, file = \"data/data.Rdata\")"
  },
  {
    "objectID": "portfolio/posts/datafest (2024)/index.html#modeling",
    "href": "portfolio/posts/datafest (2024)/index.html#modeling",
    "title": "Mizzou Datafest 2024: Best-in-Show Submission",
    "section": "Modeling",
    "text": "Modeling\nFor our exploratory model, we created an extreme gradient boosted classification tree utilizing the XGboost engine. In particular, we utilized the model to predict if a student recived a succeeding/passing grade (EOC &gt; 0.6) or failing grade in their CourseKata material. Due to time constraints in training time, we were less worried about model performance, and more interested in using the model to identify what variables are most important in student success.\n\nLibraries\nWe utilized the tidymodels package and framework to train and test our model. Other packages such as caret, data.table, and kableExtra helped aid us in model grading and outputting result tables neatly.\n\n\nShow Code\n#libs\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(caret)\nlibrary(pROC)\nlibrary(data.table)\nlibrary(kableExtra)\nlibrary(wesanderson)\nlibrary(vip)\n\n\n\n\nData Budgeting\nUsing tidymodels’s built in tools, we budgeted our data by splitting our data set into training and testing sets (70%/30%) via stratifying by our response variable. Also note that we created cross-validated data folds of our training set (10 folds - the default for the vfold_cv( ) function) also stratified by our response variable. We will utilize these training folds later during model tuning to determine the best combination of hyperparameters for our model.\n\n\nShow Code\nset.seed(123)\nDATA_SPLIT &lt;- DATA %&gt;%\n  initial_split(strata = success)\n\nDATA_TRAIN &lt;- training(DATA_SPLIT)\nDATA_TEST &lt;- testing(DATA_SPLIT)\n\nset.seed(234)\nDATA_folds &lt;- vfold_cv(DATA_TRAIN, strata = success)\nDATA_folds\n\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits             id    \n   &lt;list&gt;             &lt;chr&gt; \n 1 &lt;split [6568/731]&gt; Fold01\n 2 &lt;split [6568/731]&gt; Fold02\n 3 &lt;split [6568/731]&gt; Fold03\n 4 &lt;split [6569/730]&gt; Fold04\n 5 &lt;split [6569/730]&gt; Fold05\n 6 &lt;split [6569/730]&gt; Fold06\n 7 &lt;split [6570/729]&gt; Fold07\n 8 &lt;split [6570/729]&gt; Fold08\n 9 &lt;split [6570/729]&gt; Fold09\n10 &lt;split [6570/729]&gt; Fold10\n\n\n\n\nMaking Our “Recipe”\nThe last bit of data preparation that needed to be done was to create a tidymodels recipe. This recipe will tell our model how it should read and use the data we feed it.\n\n\nShow Code\nDATA_rec &lt;-\n  recipe(success ~ ., data = DATA_TRAIN) %&gt;%\n  step_unknown(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)\n\nprep(DATA_rec) # checking prep\n\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 9\n\n\n\n\n\n── Training information \n\n\nTraining data contained 7299 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Unknown factor level assignment for: class_id, ... | Trained\n\n\n• Dummy variables from: class_id and chapter_number, ... | Trained\n\n\nThis recipe in particular tells our model…\n\nTo predict the variable “success” utilizing all other variables in the training data set.\nTo create an “unknown” level in any nominal/factor predictor to replace any missing values within the variable.\nTo make dummy, “one hot,” variables for each level of every nominal/factor predictor.\n\n\n\nModel Specifications\nBefore we began tuning our model, we outlined the model specifications to tidymodels.\n\n\nShow Code\nxgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    tree_depth = tune(),\n    learn_rate = tune(),\n    loss_reduction = tune()) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\n\nThe chunk above specifics to tidymodels that we…\n\nare creating a gradient boosted classification tree using the XGboost engine\nare wanting to tune the following xgboost hyperparameters…\n\ntrees\nmin_n\nmtry\ntree_depth\nlearn_rate\nloss_reduction\n\n\n\n\nModel Workflow\nLastly, we combined our data recipe and model specifications into a single tidymodels workflow.\n\n\nShow Code\n#workflow\nxgb_workfl &lt;- workflow(DATA_rec, xgb_spec)\n\n\n\n\nModel Tuning & Racing\nTo tune our model and find an optimal set of hyperparameters, we utilized a model race (ANOVA) on model accuracy. Racing our models cut down on overall training and tuning time because, instead of allowing inoptimal hyperparameter combinations to continue training on folds, they were eliminated from the race. Thus, allowing more computational power be used on the models that were preforming accurately. Note that in order to speed up the race, we did utilize the doParallel library. This library allows for parallel sessions of R to run on a singular device at once, drastically increasing model tuning speed.\nIf given longer than the 22 hour time frame, we would have considered using a superior tuning method such as a Bayesian optimizer; however, an ANOVA race allowed us to race/tune hyperparameter combinations significantly faster. Even if the model didn’t utilize the “absolute optimal hyperparameters,” the ANOVA race allowed us to generate an accurate model within the short work window.\n\n\nShow Code\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\nset.seed(345)\nxgb_rs &lt;- tune_race_anova(\n  xgb_workfl,\n  resamples = DATA_folds,\n  grid = 20,\n  metrics = metric_set(accuracy),\n  control = control_race(verbose_elim = TRUE)\n)\n\n\nℹ Evaluating against the initial 3 burn-in resamples.\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nℹ Racing will maximize the accuracy metric.\nℹ Resamples are analyzed in a random order.\nℹ Fold10: 12 eliminated; 8 candidates remain.\n\nℹ Fold07: 2 eliminated; 6 candidates remain.\n\nℹ Fold03: 3 eliminated; 3 candidates remain.\n\nℹ Fold05: 0 eliminated; 3 candidates remain.\n\nℹ Fold09: 0 eliminated; 3 candidates remain.\n\nℹ Fold04: 0 eliminated; 3 candidates remain.\n\nℹ Fold06: 0 eliminated; 3 candidates remain.\n\n\nDue to time constraints of the project, we only utilized a training grid size of 20. A larger grid size could have yielded better models, but since we were able to find a satisfactory model within a grid of 20, we decided to not increase the number of models.\n\n\nRace Results\n\n\nShow Code\nrace &lt;- plot_race(xgb_rs) \n\nrace +\n  labs(title = \"Model Race (ANOVA)\",\n        y = \"Model Accuracy\") +\n  theme_minimal() +\n  theme(plot.title = (element_text(hjust = 0.5)))\n\n\n\n\n\n\n\n\n\nBased on the outcome of our race, 5 of the 20 models completed all 10 folds with a comparable and acceptable accuracy.\n\n\nShow Code\nshow_best(xgb_rs) %&gt;% \n  kable(caption = \"Best Models\") %&gt;%\n  kable_styling()\n\n\n\nBest Models\n\n\nmtry\ntrees\nmin_n\ntree_depth\nlearn_rate\nloss_reduction\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\n50\n1412\n4\n12\n0.0033944\n5.7180749\naccuracy\nbinary\n0.7612016\n10\n0.0053339\nPreprocessor1_Model14\n\n\n40\n1649\n17\n14\n0.0054080\n0.0019212\naccuracy\nbinary\n0.7607936\n10\n0.0046129\nPreprocessor1_Model12\n\n\n68\n1252\n20\n3\n0.0124206\n0.0001269\naccuracy\nbinary\n0.7566831\n10\n0.0041891\nPreprocessor1_Model19\n\n\n\n\n\n\n\nOf the five models that finished, the best had a mean accuracy of 0.76. Next we will train the highest preforming model on the entire training data set. The model’s accuracy may increase/decrease when the model is trained on the entire, unfolded, training data.\n\n\nTraining Final/Best Model\nBased on our race, the best model (model 12) had the following parameters:\n\nmtry = 40\ntrees = 1649\nmin_n = 17\ntree_depth = 14\nlearn_rate = 0.0054080\nloss_reduction = 0.0019212\n\nInstead of entering these values manually, we utilized tidymodels to simply extract the best model and retrain it using our entire training data set.\n\n\nShow Code\nxgb_last &lt;- xgb_workfl %&gt;%\n  finalize_workflow(select_best(xgb_rs, metric = \"accuracy\")) %&gt;%\n  last_fit(DATA_SPLIT)\n\n\n\n\nGrading Final Model\n\n\nShow Code\nxgb_last$.metrics[[1]] %&gt;%\n  kable(caption = \"Best Model Metrics\") %&gt;%\n  kable_styling()\n\n\n\nBest Model Metrics\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\naccuracy\nbinary\n0.7814297\nPreprocessor1_Model1\n\n\nroc_auc\nbinary\n0.8478536\nPreprocessor1_Model1\n\n\nbrier_class\nbinary\n0.1561358\nPreprocessor1_Model1\n\n\n\n\n\n\n\nBased on our final training, our best model was able to obtain an 0.786 accuracy and an roc_auc of 0.8473478.\n\n\nShow Code\n ROC_graph &lt;- xgb_last %&gt;%\n  collect_predictions() %&gt;%\n  roc_curve(success, .pred_F) %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_line(size = 1.5, color = \"#5abcd6\") +\n  geom_abline(\n    lty = 2, alpha = 0.5,\n    color = \"gray50\",\n    size = 1.2\n  ) +\n  labs(title = \"ROC for Exploratory Model\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n \n ROC_graph\n\n\n\n\n\n\n\n\n\nOur model was able to produce an roc_auc of 0.8473478. This value is within the range of a good auc (0.8 - 0.9) which means that our model is a decent classifier for if a student is succeeding or failing in their CourseKata material.\nA confusion matrix for our model’s predictions can reveal even more information about our model’s performance.\n\n\nShow Code\nDATA_pred_val &lt;- collect_predictions(xgb_last)$.pred_class\n\nmodel &lt;- extract_workflow(xgb_last)\n\nDATA_act &lt;- DATA_TEST$success\n\ncm &lt;- confusionMatrix(DATA_pred_val, DATA_act)\n\ncm\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    F    P\n         F  772  263\n         P  269 1130\n                                          \n               Accuracy : 0.7814          \n                 95% CI : (0.7645, 0.7977)\n    No Information Rate : 0.5723          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.5532          \n                                          \n Mcnemar's Test P-Value : 0.8284          \n                                          \n            Sensitivity : 0.7416          \n            Specificity : 0.8112          \n         Pos Pred Value : 0.7459          \n         Neg Pred Value : 0.8077          \n             Prevalence : 0.4277          \n         Detection Rate : 0.3172          \n   Detection Prevalence : 0.4252          \n      Balanced Accuracy : 0.7764          \n                                          \n       'Positive' Class : F               \n                                          \n\n\nBased on metrics derived from the confusion matrix, one can see that our model’s specificity is at 82.05% while our sensitivity is at 73.97%. In other words, our model is better at predicting if a student is successful versus if a student is failing.\n\n\nIdentifying Key Features Within the Model\nRecall that the main purpose of our modeling efforts was to create an exploratory model that allowed us to identify key variables and their effects on student success. Now that we had created a satisfactory model, we utilized the model to identify which variables were important in the model’s classification methodology.\nUtilizing the vip library, we extracted the top 10 most important features from the model and identified which ones we wanted to perform further analysis on.\n\n\nShow Code\nimportance_graph &lt;- extract_workflow(xgb_last) %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(geom = \"col\", num_features = 10, mapping = aes(fill = Variable))\n\nimportance_graph +\n  labs(title = \"Top Ten Important Variables for Pass/Fail\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"none\") +\n  scale_fill_manual(values = wes_palette(name = \"Darjeeling1\", n = 10, type = \"continuous\"))\n\n\n\n\n\n\n\n\n\nBased on the importance graph, it appeared that the ensemble model found engaged_sum, avg_attempt, insitution_id_I.02 andchapter_number_X1 to be the most important in determining if a student was passing or failing their CourseKata material.\nWith our important variables identified, we committed any further analysis to identifying how student engagement, average question attempt, institution, and chapter number affected student EOC scores."
  },
  {
    "objectID": "portfolio/posts/datafest (2024)/index.html#summarizing-findings-conclusions",
    "href": "portfolio/posts/datafest (2024)/index.html#summarizing-findings-conclusions",
    "title": "Mizzou Datafest 2024: Best-in-Show Submission",
    "section": "Summarizing Findings & Conclusions",
    "text": "Summarizing Findings & Conclusions\n\nLibraries\nTo analyze, graph, and understand our important variables, we used the tools found within the tidyverse package. Additionally, the colors for our graphs were sourced from the Darjeeling1 palette which can be found within the Wes Anderson palette package.\n\n\nShow Code\nlibrary(tidyverse)\nlibrary(wesanderson)\n\ndesired_color &lt;- wes_palette(\"Darjeeling1\")[1]  # getting graph colors\ndesired_color2 &lt;- wes_palette(\"Darjeeling1\")[5]  \n\n\n\n\nAnalysis of Important Variables\n\nStudent Determined Variables\nTwo of the variables identified by our model were student determined/controlled variables. In other words, these variables are direct measurements of individual student behavior.\n\nTotal Student Engagement\nStudents’ total time spent engaged with the CourseKata material seems to play a significant part in if they are successful academically. In particular, and unsurprising, it appears to be the case that students who spend more time engaged with CourseKata material are successful in the passing the material. Thus, overall lack of engagement with the material should serve as a warning sign to instructors that a student may be failing or struggling in the CourseKata material.\n\n\nShow Code\nbox_engaged_sum &lt;- DATA %&gt;%\n  ggplot(aes(x = success, y = engaged_sum)) +\n  labs(title = \"Pass/Fail and Student Engagement\",\n       x = \"Pass/Fail\",\n       y = \"Total Engaged Minutes\\n(log scale)\") +\n  scale_y_continuous(trans = scales::pseudo_log_trans(base = 10)) +\n  geom_violin(aes(color = success)) +\n  geom_boxplot(width = 0.4, aes(fill = success), outliers = F) +\n  scale_fill_manual(values = c(desired_color, desired_color2)) +  \n  coord_flip() +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"top\")\nbox_engaged_sum\n\n\n\n\n\n\n\n\n\nNote that the horizontal axis utilizes logarithmic scaling.\n\n\n\nAverage Question Attempt\nA student’s average number of question attempts also appears to be related to their EOC and thus chances of passing/failing the CourseKata material. Specifically, there appears to be an identifiable cut off around where the average number of attempts is more than three (black line on graph) where there are significant more students who are failing than passing the material. Again, and perhaps a better indicator than total engagement time, average number of question attempts could serve as an indicator to instructors that a student may be struggling with the course material.\n\n\nShow Code\navg_attempt_hist &lt;- DATA %&gt;%\n  ggplot(aes(x = (avg_attempt), y = EOC, color = fct_rev(success))) +\n  geom_jitter() +\n  scale_x_continuous(trans = scales::pseudo_log_trans(base = 10)) +\n  labs(title = \"Average Number of Attempts per Question by EOC\",\n       x = \"Average Number of Attempts per Question\\n(log scale)\",\n       color = \"Pass/Fail\") +\n  geom_vline(xintercept = 3) +\n  scale_color_manual(values = c(desired_color2, desired_color)) +  \n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"top\")\navg_attempt_hist\n\n\n\n\n\n\n\n\n\nNote that the horizontal axis utilizes logarithmic scaling.\n\n\nEnvironment Determined Variables\nTwo of the variables identified by our model were environment determined/controlled variables. In other words, these variables are elements of the course that individual students have little to no control over (e.g. instructor/institution, book version, course material itself, etc).\n\n\nInstitutions\nOur model identified that students at different institutions preformed significantly different in the CourseKata material. Specifically, a large majority of students from institutions 2, 7, and 8 passed the CourseKata material, and half or more of students from institutions 3, 4, 5, 6 did not pass. Institutions were kept anonymous in the data release used for this analysis, thus we have no insights to why these different institution preformed at such different levels. However, it is recommended that if the CourseKata material is to improve, internal teams should investigate the different well and poor preforming students at each institution and attempt to identify trends.\n\n\nShow Code\nStackbarsInst &lt;- DATA %&gt;%\n  ggplot(aes(x = institution_id, fill = fct_rev(success))) +\n  geom_bar(position = \"fill\",\n           colour = \"black\",\n           size = 0.35) +\n  coord_flip() +\n  labs(title = \"Institutions and Pass/Fail Proportions\",\n       x = \"Institution\",\n       y = \"Proportion of Students\",\n       fill = \"Pass/Fail\") + \n  scale_fill_manual(values = c(desired_color2, desired_color)) +  \n  geom_hline(yintercept = 0.5, linetype=\"dotted\", size = 1) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"top\")\nStackbarsInst\n\n\n\n\n\n\n\n\n\n\n\nBook Chapter\nBook chapter also played a significant impact on student performance. It appears that as the material advances, student performance degrades. Notably, there is the greatest amount of degradation in chapters 10-13 were the majority of students go from passing to failing the material. Further investigation needs to be done into why this academic attrition occurs, and how to prevent it in future course editions and volumes.\n\n\nShow Code\nStackbarsCh &lt;- DATA %&gt;%\n  ggplot(aes(x = chapter_number, fill = fct_rev(success))) +\n  geom_bar(position = \"fill\",\n           colour = \"black\",\n           size = 0.35) +\n  coord_flip() +\n  labs(title = \"Book Chapter and Pass/Fail Proportions\",\n       x = \"Book Chapter\",\n       y = \"Proportion of Students\",\n       fill = \"Pass/Fail\") + \n  scale_fill_manual(values = c(desired_color2, desired_color)) +  \n  geom_hline(yintercept = 0.5, linetype=\"dotted\", size = 1) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"top\")\nStackbarsCh\n\n\n\n\n\n\n\n\n\n\n\nBook Version\nThe xGboost classifier did identify book version/volume as a variable of importance. Over half the students utilizing v5.0 and v5.1.1 are failing the material, and over 60% of the students utilizing v5.0-exp1 and v5.2 are passing the material. It should be noted that book version may be confound with other variables such as instructor, and institution. There was not enough data within the sample given to us to confirm or deny this, thus internal CourseKata teams should confirm/deny the effects of book version on student performance.\n\n\nShow Code\nStackbarsVer &lt;- DATA %&gt;%\n  ggplot(aes(x = release, fill = fct_rev(success))) +\n  geom_bar(position = \"fill\",\n           colour = \"black\",\n           size = 0.35) +\n  coord_flip() +\n  labs(title = \"Book Version and Pass/Fail Proportions\",\n       x = \"Book Version\",\n       y = \"Proportion of Students\",\n       fill = \"Pass/Fail\") + \n  scale_fill_manual(values = c(desired_color2, desired_color)) +  \n  geom_hline(yintercept = 0.5, linetype=\"dotted\", size = 1) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"top\")\nStackbarsVer\n\n\n\n\n\n\n\n\n\nIt should be noted that book version was less important within the model than the other variables investigated; however, our team still deemed that analysis of this variable may provided insights on how CourseKata could be improved.\n\n\n\nNext Steps and Final Recommendations\nOur recommendations to the internal teams of CourseKata are…\n\nto remove or rework the subjective “pulse” questions, as they had no statistical relevance or significance.\nutilize total time engaged and average number of question attempts as early warning indicators for failing students. If instructors are given early warning of a student’s performance, they may be able to better help and aid that student in their learning.\nto investigate the discrepancies among the general performance of student bodies at different institutions.\nconfirm the effects of different textbook volumes/editions on student performance. We recommend checking for confounding effects/factors like institution, instructor, class grade level, etc."
  },
  {
    "objectID": "portfolio/posts/datafest (2024)/index.html#extra-content",
    "href": "portfolio/posts/datafest (2024)/index.html#extra-content",
    "title": "Mizzou Datafest 2024: Best-in-Show Submission",
    "section": "Extra Content",
    "text": "Extra Content\nUnrelated to our main research goals, our team also tried to identify trends within students’ written responses. This avenue of analysis was abandoned when our model was able to identify more succinct and significant findings.\nWith the time dedicated to the analysis of student written responses, our team member was able to do some text mining and create a word cloud of the most common words utilized in students’ responses.\n\nText Mining and Word Cloud\n\n\nShow Code\n#install.packages(\"wordcloud\")\nlibrary(wordcloud)\n\n#install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\n\n#install.packages(\"wordcloud2\")\nlibrary(wordcloud2)\n\n#install.packages(\"tm\")\nlibrary(tm)\nlibrary(tidyverse)\nresponses &lt;- read.csv(\"data/responses.csv\")#View(responses_sample)\n\npass_responses &lt;- responses %&gt;% \n  mutate(points_possible = as.numeric(points_possible)) %&gt;% \n  mutate(points_earned = as.numeric(points_earned)) %&gt;% \n  filter(!is.na(points_earned)) %&gt;% \n  filter(!is.na(points_possible)) %&gt;% \n  mutate(perc_score = points_possible/points_earned) %&gt;% \n  filter(perc_score &gt; .6) %&gt;% \n  filter(institution_id == \"97aebe75-a051-4bff-a2c0-1d53eb5d9498\")\n\n\n\n\nShow Code\n# Making DF for word clouds\n\n# Pre word cloud\ncorpus = Corpus(VectorSource(pass_responses$response))\n\ncorpus &lt;- corpus %&gt;% \n  tm_map(removeNumbers) %&gt;%\n  tm_map(removePunctuation) %&gt;%\n  tm_map(stripWhitespace) %&gt;%\n  tm_map(content_transformer(tolower)) %&gt;%\n  tm_map(removeWords, stopwords(\"english\")) %&gt;%\n  tm_map(removeWords, stopwords(\"SMART\"))\n\ntdm = TermDocumentMatrix(corpus) %&gt;% \n  as.matrix()\n\nwords = sort(rowSums(tdm), decreasing = TRUE)\n\npre_WCdf = data.frame(words = names(words), freq = words)\n\n\n# Color Palettes\npre_WCcolors = c(\"#510C76\", \"#00A8E2\", \"#87714D\")\npre_WCbkgd = \"#FFFFFF\"\npost_WCcolors = c(\"#FFFFFF\", \"#510C76\", \"#87714D\")\npost_WCbkgd = \"#00A8E2\"\n\n#rm unneeded vars\nrm(corpus, tdm, words)\n\nWC_Pre &lt;- wordcloud2(pre_WCdf,\n           color = rep_len(pre_WCcolors, nrow(pre_WCdf)),\n           backgroundColor = pre_WCbkgd,\n           fontFamily = \"AppleMyungjo\",\n           size = .62,\n           rotateRatio = 0)\n\n#Final wordcloud\nWC_Pre\n\n\n\n\n\n\nWith more time, we may have be able to conduct meaningful sediment analysis on student responses; however, there was simply too little time during the datathon."
  },
  {
    "objectID": "portfolio/posts/Playfair/index.html",
    "href": "portfolio/posts/Playfair/index.html",
    "title": "Reconstruction of 19th Century Data Visualization",
    "section": "",
    "text": "Recently in one of my data visualization classes, we were tasked with recreating a 19th century William Playfair Graph using modern day visualization software.\nWilliam Playfair is credited as being one of the founders of the use of graphical methods in statistics. In his “Chart Representing the Extent, Population and Revenues of the Principal Nations in Europe,” Playfair attempted to visualize comparative taxation among the European nations, specifically trying to rely the story that England taxed more heavily than other European nations. Playfair even writes in supporting text that the graph “shows whether in proportion to its population the country is burdened with heavy taxes or otherwise.”\n\n\n\nPlayfair’s Orginal Visualization\n\n\nPlayfair’s visualization effectively leverages the use of vertical lines to demonstrate discrepancies in population and taxation. The lines used in his graphic are akin to modern day use of bars in bar plots, in the manner that the vertical height/final resting place of the line/bar designates the value of the mapped metric. Since both population and taxation are mapped to similarly scales, his visualization essentially takes advantage of a dual y-axis, allowing for the viewer to directly and easily compare a county’s population to its taxation. Additionally, a viewers perception is explicitly drawn towards any contrasts in tax and population via the dashed segment that connects a country’s measures. This connecting segment not only draws perception, but also helps viewers distinguish between countries via the creation of perceptual groupings.\nAlthough not aiding in Playfair’s goal of demonstrating unfair taxation, his use of circles to represent a country’s square area is effective. The area of the circles appear to be properly mapped to a country’s square area, and not mapped to the radius of the circles. Furthermore, the circles are plotted horizontally in descending order, establishing a clear hierarchy. However, the circles do distract from Playfair’s primary thesis. The size, coloring, and “weight” of the circles draws immediate attention towards the bottom of the graph - which is away from the lines/measures of population and taxation. Without knowing Playfair’s intentions, one may assume that he is attempting to assert an argument not about the relation of population and taxation, but rather the relationship between a country’s area and population, or a country’s area and taxation, and/or any combination of the three."
  },
  {
    "objectID": "portfolio/posts/Playfair/index.html#introduction",
    "href": "portfolio/posts/Playfair/index.html#introduction",
    "title": "Reconstruction of 19th Century Data Visualization",
    "section": "",
    "text": "Recently in one of my data visualization classes, we were tasked with recreating a 19th century William Playfair Graph using modern day visualization software.\nWilliam Playfair is credited as being one of the founders of the use of graphical methods in statistics. In his “Chart Representing the Extent, Population and Revenues of the Principal Nations in Europe,” Playfair attempted to visualize comparative taxation among the European nations, specifically trying to rely the story that England taxed more heavily than other European nations. Playfair even writes in supporting text that the graph “shows whether in proportion to its population the country is burdened with heavy taxes or otherwise.”\n\n\n\nPlayfair’s Orginal Visualization\n\n\nPlayfair’s visualization effectively leverages the use of vertical lines to demonstrate discrepancies in population and taxation. The lines used in his graphic are akin to modern day use of bars in bar plots, in the manner that the vertical height/final resting place of the line/bar designates the value of the mapped metric. Since both population and taxation are mapped to similarly scales, his visualization essentially takes advantage of a dual y-axis, allowing for the viewer to directly and easily compare a county’s population to its taxation. Additionally, a viewers perception is explicitly drawn towards any contrasts in tax and population via the dashed segment that connects a country’s measures. This connecting segment not only draws perception, but also helps viewers distinguish between countries via the creation of perceptual groupings.\nAlthough not aiding in Playfair’s goal of demonstrating unfair taxation, his use of circles to represent a country’s square area is effective. The area of the circles appear to be properly mapped to a country’s square area, and not mapped to the radius of the circles. Furthermore, the circles are plotted horizontally in descending order, establishing a clear hierarchy. However, the circles do distract from Playfair’s primary thesis. The size, coloring, and “weight” of the circles draws immediate attention towards the bottom of the graph - which is away from the lines/measures of population and taxation. Without knowing Playfair’s intentions, one may assume that he is attempting to assert an argument not about the relation of population and taxation, but rather the relationship between a country’s area and population, or a country’s area and taxation, and/or any combination of the three."
  },
  {
    "objectID": "portfolio/posts/Playfair/index.html#re-creation",
    "href": "portfolio/posts/Playfair/index.html#re-creation",
    "title": "Reconstruction of 19th Century Data Visualization",
    "section": "Re-creation",
    "text": "Re-creation\nIn attempt to recreate the visualization using modern day software, I utilized R an the ggplot, ggthemes, and ggforce libraries. Ggplot does most of the heavy lifting for the visualization; however, ggthemes allows me to more closely match the original stylization of Playfair’s chart, and ggforce has a circle geom for ggplot. It may be possible to recreate the chart without the use of ggforce’s geom_circle, but it makes the task much simpler.\n\nData Preparation\nThe first, and most important step in the recreation process was preparing the data. For the assignment, I was given a simple tidy dataframe that contained four columns of information:\n\nCountry Name\nCountry Area\nCountry Population\nCountry Taxation\n\nIn order to properly map the elements of the graph I had to engineer:\n\nCountry Circle Radius\nScaled Country Circle Radius\nCountry Circle Center\nCountry Circle Labels\nLabel Adjustment/Padding\nCountry Faring Type (land/sea)\nPlot Title with Proper Formatting\n\n\nData Preperation Code\n\n\nShow Code\nDATA &lt;- DATA %&gt;%\n  mutate(Country = as.factor(Country)) %&gt;%\n  mutate(r = sqrt(Area / 3.14))\n\nDATA &lt;- DATA %&gt;%\n  mutate(scaledR = r / max(DATA$r, na.rm = TRUE) * 10)\n\nDATA$center[1] &lt;- DATA$scaledR[1]\nspaceBetweenCountries &lt;- 2\n\nfor (x in 2:nrow(DATA)) {\n  DATA$center[x] &lt;- DATA$center[x - 1] + \n                    DATA$scaledR[x - 1] + \n                    spaceBetweenCountries +\n                    DATA$scaledR[x]\n}\n\nDATA &lt;- DATA %&gt;%\n  mutate(circleLabels = str_replace_all(Country, \" \", \"\\n\"))\n\nDATA$circleLabels[11] &lt;- \"Countries under the\\nDominion of France\"\n\nDATA$circLabelAdj &lt;- c(0, 0,\n                       2.2, 2.7, 2, 2.5, 2.2, 2, 2.2, 2.2, 5, 2.3)\n\nDATA$Faring &lt;- c(\"land\", \"land\", \"land\", \"sea\", \"land\", \"land\", \"land\", \"land\",\n                 \"land\", \"sea\", \"land\", \"land\")\n\n# creating plot title w/ formating\nplotTitle &lt;- expression(paste(italic(\"Chart\"),\n                              \" Representing the \",\n                              bold(\"Extent, Population & Revenues, \"),\n                              \"of the \", italic(\"Principal Nations \"),\n                              \"in \",\n                              bold(\"Europe, \"),\n                              \"after the \",\n                              italic(\"Division \"),\n                              \"of \",\n                              bold(\"Poland & Treaty \"),\n                              \"of \",\n                              bold(\"Luneville.\")))\n\n\n\n\n\nGraphing\nAfter feature engineering and data preparation was complete, it became a manner of mapping the correct data frame features to the correct graphical elements and layers. I spent significant time on the stylizing elements of the graph so that it would closely match Playfair’s original style. I made sure to include:\n\nSerif Fonts\nDual, Repeated y-axis labels\nColor matching (the best I could) to the faded colors of the orignal\nAxis and element labels\n\n\nGraphing Code & Final Reconstruction\n\n\nShow Code\nDATA %&gt;%\n  ggplot() +\n   geom_circle(aes(x0 = center, y0 = 0, r = scaledR, fill = Faring), # circles\n               alpha = 0.5) + \n   geom_segment(aes(x = center - scaledR + 0.25, y = 0, yend = Population),\n                color = \"red\", linewidth = 1, alpha = 0.45) + # red lines\n   geom_segment(aes(x = center - scaledR, y = 0, yend = Population)) +\n   geom_segment(aes(x = center + scaledR - 0.25, y = 0, yend = Taxation),\n                color = \"yellow\", linewidth = 1, alpha = 0.45) + # yellow lines\n   geom_segment(aes(x = center + scaledR, y = 0, yend = Taxation)) +\n   geom_segment(aes(x = center - scaledR, xend = center + scaledR, # connect\n                    y = Population, yend = Taxation), linetype = \"dotted\") +\n   geom_text(data = DATA[1:2,], # Russia & Turkish Labels\n             aes(x = center, y = 0 - scaledR, label = Country),\n             vjust = 1.25, size = 3.5, family = \"serif\", fontface = \"bold\") +\n   geom_text(data = DATA[3:12,], # Country labels for remaining\n             aes(x = center, y = 0 - scaledR - circLabelAdj,\n                 label = circleLabels),\n             angle = 90, hjust = 0.5, size = 3, family = \"serif\",\n             fontface = \"bold\") +\n   geom_text(data = DATA[1,], # Russia Sq mile\n             aes(x = center, y = 0 - scaledR, label = paste(Area,\n                                                            \" Square Miles\")),\n             vjust = 3.5, size = 2.25, family = \"serif\", fontface = \"bold\") +\n   geom_text(data = DATA[2,], # Turkish Sq mile\n             aes(x = center, y = 0, label = paste(Area,\" Sq Miles\")),\n             vjust = 1.25, size = 2.25, family = \"serif\", fontface = \"bold\") +\n   geom_text(data = DATA[c(3,5),], # Swedish & French Sq mile\n             aes(x = center, y = 0, label = paste(Area,\"\\nSq Miles\")),\n             vjust = 1.25, size = 1.75, family = \"serif\", fontface = \"bold\") +\n   geom_text(data = DATA[c(4, 6, 7, 8, 9, 10, 11, 12),],#miles for everyone else\n             aes(x = center, y = 0, label = paste(Area)),\n             vjust = 1.25, size = 1.75, family = \"serif\", fontface = \"bold\") +\n   geom_hline(yintercept = seq(0, 30, 10)) +\n   geom_hline(yintercept = seq(0, 30, 1), alpha = 0.15) +\n   scale_y_continuous(breaks = seq(0, 30, 1),\n                      sec.axis = dup_axis(),\n                      labels = c(\"\", seq(1, 30, 1))) +\n   scale_fill_manual(values = c(\"darkgreen\", \"firebrick\")) +\n   labs(title = plotTitle,\n        x = \"\",\n        y = \"\") +\n   coord_equal() +\n   theme_solarized() +\n   theme(axis.text.y = element_text(size = 9),\n         panel.grid.major.y = element_blank(),\n         panel.grid.minor.y = element_blank(),\n         panel.grid.major.x = element_blank(),\n         panel.grid.minor.x = element_blank(),\n         axis.text.x = element_blank(),\n         axis.ticks.x = element_blank(),\n         plot.title = element_text(size = 12, hjust = 0.5,\n                                   family = \"serif\", color = \"black\"),\n         text = element_text(family = \"serif\", color = \"black\", face = \"bold\"),\n         legend.position = \"none\")"
  },
  {
    "objectID": "portfolio/posts/Playfair/index.html#new-data-visulization",
    "href": "portfolio/posts/Playfair/index.html#new-data-visulization",
    "title": "Reconstruction of 19th Century Data Visualization",
    "section": "New Data Visulization",
    "text": "New Data Visulization\nAfter reconstructing Playfair’s chart, I spent some time recreating it using modern day visualization methods and techniques.\nPlayfair’s main thesis was to show that England had unfair taxation for its population. Therefore the primary elements of the visualization should attempt to reflect this relationship. Thus, I choose the x and y axis to represent population and taxation (respectively). This allows a viewer to directly see and contrast the population and taxation values of the countries shown, and identify outliers. Immediately, one is able to see that this visualization does support Playfair’s original thesis, as Britain and Ireland (England) break pattern and show a substantially greater taxation for their population.\nI attempted to preserve all other original elements of Playfair’s graphic: circle area for country area, labels for countries and square miles, colors for if a country’s economy is land or sea faring, title, font type/face, and a yellowed background. I preserved all of these elements because it was my goal to demonstrate that changing/remapping even just one or two elements can make a significantly more concise visualization.\n\nData Preperation\n\n\nShow Code\n# data prep\nDATA &lt;- DATA %&gt;%\n  mutate(dotLabels = paste(Country, \"\\n\", Area, \" Sq Mi\"))\n\n# plot title w/ formatting\nplotTitle &lt;- expression(atop(paste(italic(\"Chart\"),\n                                   \" Representing the \",\n                                   bold(\"Extent, Population & Revenues, \"),\n                                   \"of the \",\n                                   italic(\"Principal\")),\n                             paste(italic(\"Nations \"),\n                                   \"in \",\n                                   bold(\"Europe, \"),\n                                   \"after the \",\n                                   italic(\"Division \"),\n                                   \"of \",\n                                   bold(\"Poland & Treaty \"),\n                                   \"of \",\n                                   bold(\"Luneville.\"))))\n\n\n\n\nCode for Graphic & New Graph\n\n\nShow Code\nDATA %&gt;%\n  ggplot(aes(x = Population, y = Taxation)) +\n  geom_circle(aes(x0 = Population, y0 = Taxation, r = scaledR/5, fill = Faring),\n              alpha = 0.5) +\n  geom_text_repel(aes(label = dotLabels),\n                  size = 2, family = \"serif\", fontface = \"bold\",\n                  seed = 5) +\n  labs(size = \"\",\n       title = plotTitle,\n       caption = \"Note that Britain & Ireland has a much higher tax to population ratio\") +\n  scale_fill_manual(values = c(\"darkgreen\", \"firebrick\")) +\n  guides(size = \"none\") +\n  coord_equal() +\n  theme_solarized() + \n  theme(legend.position = \"bottom\",\n        legend.title = element_text(family = \"serif\", color = \"black\"),\n        axis.title = element_text(family = \"serif\", color = \"black\",\n                                  face = \"bold\"),\n        plot.title = element_text(size = 12, hjust = 1,\n                                   family = \"serif\", color = \"black\"),\n        plot.caption = element_text(hjust = 0.5,\n                                    family = \"serif\", color = \"black\"),\n        text = element_text(family = \"serif\", color = \"black\", face = \"bold\"))\n\n\n\n\n\n\n\n\n\nIf I were to improve the visualization further I would change the font type/face. Serif fonts, especially in small sizes like shown in both the original and new graphic, are difficult to read. A sans serif font would be much easier for viewers to read. I would also remove the yellowed background as it holds no true purpose to the thesis, and I would change the title to be more concise. Lastly, I would take effort in drawing a viewers perception to Britain and Ireland’s circle. Since the main thesis is to highlight it’s peculiarity, I would leverage something to draw viewers’ eyes towards the point (enclosing the circle in a red border or pointing a red arrow towards it are two ideas)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I am a current senior at Truman State University studying Statistics and Data Science. I have always had an affinity and deep curiosity for computers, data, and problem solving. I am currently pursing career opportunities in in data science, analytics, and engineering as I find the work exciting and rewarding."
  },
  {
    "objectID": "consulting/index.html",
    "href": "consulting/index.html",
    "title": "Consulting Projects",
    "section": "",
    "text": "This is a listing of various consulting projects that I have worked on. Unless explicitly listed or linked below, I am unable to share copies of my work due to non-disclosure agreements with my clients. If you would like to know more about any of the projects, you can reach out to me via any of my listed contact methods, or with the client or supervisor contact listed bellow."
  },
  {
    "objectID": "consulting/index.html#disclaimer",
    "href": "consulting/index.html#disclaimer",
    "title": "Consulting Projects",
    "section": "",
    "text": "This is a listing of various consulting projects that I have worked on. Unless explicitly listed or linked below, I am unable to share copies of my work due to non-disclosure agreements with my clients. If you would like to know more about any of the projects, you can reach out to me via any of my listed contact methods, or with the client or supervisor contact listed bellow."
  },
  {
    "objectID": "consulting/index.html#the-effect-of-aphasia-specific-communication-training-on-student-physicians-interactions-with-standardized-patients-portraying-non-fluent-aphasia",
    "href": "consulting/index.html#the-effect-of-aphasia-specific-communication-training-on-student-physicians-interactions-with-standardized-patients-portraying-non-fluent-aphasia",
    "title": "Consulting Projects",
    "section": "The Effect of Aphasia-Specific Communication Training on Student Physicians’ Interactions with Standardized Patients Portraying Non-Fluent Aphasia",
    "text": "The Effect of Aphasia-Specific Communication Training on Student Physicians’ Interactions with Standardized Patients Portraying Non-Fluent Aphasia\n\nDescription\nProvided expert statistical consulting and assistance to primary researchers. I aided researchers in statistical testing design and preformed the statistical analysis, testing, visualization, and content analysis for the study.\nDue the the nature of the study, I primarily utilized paired, non-parametric statistical testing on quantitative study elements, and preformed sentiment and text mining analysis for qualitative study elements.\n\nContacts\nClients: Tucker Murry (tmm3867@truman.edu)\nConsulting Supervisor: Dr. Scott K. Alberts (salberts@truman.edu, (660) 785-7649)"
  },
  {
    "objectID": "consulting/index.html#survey-of-undergradute-students-and-analysis-of-primary-deciding-factors-for-college-selection",
    "href": "consulting/index.html#survey-of-undergradute-students-and-analysis-of-primary-deciding-factors-for-college-selection",
    "title": "Consulting Projects",
    "section": "Survey of Undergradute Students and Analysis of Primary Deciding Factors for College Selection",
    "text": "Survey of Undergradute Students and Analysis of Primary Deciding Factors for College Selection\n\nDescription\nDesigned and executed a mass survey for a undergraduate student body to ascertain the primary deciding factors for college selection. Utilized non-parametric statistics to identify trends and differences within the survey results, which were summarized and visualized in a formal report to client.\nPresented main findings to client, leaders in the University, and invested stakeholders.\n\nContacts\nClients: Hayden Wilsy (hwilsey@truman.edu)\nConsulting Supervisor: Dr. Scott K. Alberts (salberts@truman.edu, (660) 785-7649)"
  },
  {
    "objectID": "consulting/index.html#updating-intake-methods-for-undergraduate-first-destination-data",
    "href": "consulting/index.html#updating-intake-methods-for-undergraduate-first-destination-data",
    "title": "Consulting Projects",
    "section": "Updating Intake Methods for Undergraduate First Destination Data",
    "text": "Updating Intake Methods for Undergraduate First Destination Data\n\nDescription\nDesigned and prototyped survey to efficiently garner a graduating undergraduate’s first destination after college. The survey was designed to improve upon a current existing data collection system by reducing time required to complete the survey, improving question clarity, and creating a better surveyee experience. The new survey drastically improved survey completion rate and validity of results.\nIn addition to the survey, cleaning scripts and code for automated reports were also provided for the client.\n\nContacts\nClients: Dr. David Lusk (dllusk@truman.edu), Joel Brumfield (jbrumfield@truman.edu, (660) 785-4237)\nConsulting Supervisor: Dr. Scott Thatcher (thatcher@truman.edu, (660) 785-4552)"
  },
  {
    "objectID": "portfolio/posts/pysparkFeatureDrift/index.html",
    "href": "portfolio/posts/pysparkFeatureDrift/index.html",
    "title": "Pyspark (Databricks) Library for Feature Drift Detection",
    "section": "",
    "text": "To maintain optimal performance, machine learning models have to be refreshed. This is especially pertinent if a model’s performance directly effects customers or business operations, as poor performance can have immediate negative consequences.\nRefreshing models takes time and computing resources, so although it is necessary, it can be costly. Thus, most companies implement strategies that establish when they choose to refresh models. Strategies such as…\n\nRegularly Scheduled Refreshes: Refreshing models regularly is great because it is easy and consistent. However, its downside can lie in the time period an organization chooses. If the time period is too large, then real world trends may change during the periods between model refreshes. Vise versa, if the time window is too small, then one is refreshing models when they don’t need to be and overusing computing resources. This strategy effectively requires a “Goldilocks” period that is neither too large or short, which can be difficult to find.\nRefreshing when Model perform Declines: Refreshing models when their performance begins to decline is a fine strategy, unless the model’s performance is crucial to maintaining the business. Depending on the use case, even slight degradation in performance can have dramatic outcomes. Additionally, model refreshes can sometimes take longer than anticipated, which could result in degrading models being in production for longer than one wants.\n\nRefreshing when Trends Change: A more proactive strategy would be to update models when the trends within data drift. Large drifts in features a model uses can signify when a model needs to be refreshed. Thus, implementing trackers and reports that indicate substantial feature drift can lead to refreshing a model before performance is impacted. However, this strategy is difficult to generalize. Data across all use-cases looks different and requires alternative methods for feature drift detection.\nA Mix of Strategies: It should be noted that the world of data is not as black and white as the options listed above, and most companies implement multiple systems and trackers for model refreshes. For example, it may be the case that a company has a policy to refresh models quarterly, but has trackers in place to send alerts when model performance declines or trends change - indicating the necessity to refresh outside of the regular schedule.\n\nThis library of functions attempts to address the third strategy listed.\nAs mentioned, identifying feature drift is a great method for indicating when a model needs to be refreshed; however, identifying feature drift is difficult to generalize across all use-cases. Thus, this library provides functions and tools that can be used to achieve a generalized feature drift detection methodology. It does this by leveraging Pyspark to distribute and parallelize the computation of robust, non-parametetric statistics that identify and measure feature drift regardless of data shape or size.\nTo view code for the library one can visit my github page of the library here."
  },
  {
    "objectID": "portfolio/posts/pysparkFeatureDrift/index.html#introduction",
    "href": "portfolio/posts/pysparkFeatureDrift/index.html#introduction",
    "title": "Pyspark (Databricks) Library for Feature Drift Detection",
    "section": "",
    "text": "To maintain optimal performance, machine learning models have to be refreshed. This is especially pertinent if a model’s performance directly effects customers or business operations, as poor performance can have immediate negative consequences.\nRefreshing models takes time and computing resources, so although it is necessary, it can be costly. Thus, most companies implement strategies that establish when they choose to refresh models. Strategies such as…\n\nRegularly Scheduled Refreshes: Refreshing models regularly is great because it is easy and consistent. However, its downside can lie in the time period an organization chooses. If the time period is too large, then real world trends may change during the periods between model refreshes. Vise versa, if the time window is too small, then one is refreshing models when they don’t need to be and overusing computing resources. This strategy effectively requires a “Goldilocks” period that is neither too large or short, which can be difficult to find.\nRefreshing when Model perform Declines: Refreshing models when their performance begins to decline is a fine strategy, unless the model’s performance is crucial to maintaining the business. Depending on the use case, even slight degradation in performance can have dramatic outcomes. Additionally, model refreshes can sometimes take longer than anticipated, which could result in degrading models being in production for longer than one wants.\n\nRefreshing when Trends Change: A more proactive strategy would be to update models when the trends within data drift. Large drifts in features a model uses can signify when a model needs to be refreshed. Thus, implementing trackers and reports that indicate substantial feature drift can lead to refreshing a model before performance is impacted. However, this strategy is difficult to generalize. Data across all use-cases looks different and requires alternative methods for feature drift detection.\nA Mix of Strategies: It should be noted that the world of data is not as black and white as the options listed above, and most companies implement multiple systems and trackers for model refreshes. For example, it may be the case that a company has a policy to refresh models quarterly, but has trackers in place to send alerts when model performance declines or trends change - indicating the necessity to refresh outside of the regular schedule.\n\nThis library of functions attempts to address the third strategy listed.\nAs mentioned, identifying feature drift is a great method for indicating when a model needs to be refreshed; however, identifying feature drift is difficult to generalize across all use-cases. Thus, this library provides functions and tools that can be used to achieve a generalized feature drift detection methodology. It does this by leveraging Pyspark to distribute and parallelize the computation of robust, non-parametetric statistics that identify and measure feature drift regardless of data shape or size.\nTo view code for the library one can visit my github page of the library here."
  },
  {
    "objectID": "portfolio/posts/pysparkFeatureDrift/index.html#library",
    "href": "portfolio/posts/pysparkFeatureDrift/index.html#library",
    "title": "Pyspark (Databricks) Library for Feature Drift Detection",
    "section": "Library",
    "text": "Library\nWhat follows in an overview of each of the functions inside of the library. These descriptions outline the math, statistics, and high-level programming concepts utilized in each function. To actually read the code and see how these functions were programed, one can visit the github page for this library here.\n\ntestNumeric\nThe testNumeric function tests numeric features for drift in shape and central tendency. It does this through a staged analysis process that weeds features as it progresses, preventing further analysis on features that are found to have not drifted from the benchmark.\nThe first stage of analysis is conducting a two-sample Kolmogorov–Smirnov test at a user defined significance level. The Kolmogorov–Smirnov test is a non-parametric statistical test that tests whether two features came from the same probability distribution. If a feature passes the test (i.e. the feature’s distribution is different from the benchmark distribution), then it passes onto the next stage of analysis. If a feature fails to pass the test (i.e. the feature’s distribution is identical to the benchmark distribution) then analysis on that feature is halted. The function also allows for the use of significance adjustment. If specified in the parameterization, the function will apply a Bonferroni significance adjustment on the p-values of the Kolmogorov–Smirnov test, where n is the number of numeric features being tested by the function. It should be noted that the Kolmogorov–Smirnov test is computationally robust, as one is able to leverage Spark Clusters to compute and compare cumulative density functions distributedly. This significantly reduces the time it would take for a single machine to calculate the test statistic and p-value derived from a Kolmogorov–Smirnov test.\nDue to the large nature of real-world datasets, the Kolmogorov–Smirnov test has a tendency to identify small changes in features that would typically be considered nonsubstantive. Consequently, it is necessary to analyze the effect size of differences detected by the Kolmogorov–Smirnov test. Robust, standard central differences are calculated for every feature that passes a Kolmogorov–Smirnov test, to quantify effect size in central tendency changes. The standard central differences calculated are modeled after Cohen’s d (standard mean difference), but utilize Harrell-Davis quantiles to compute median difference and a pooled, absolute median dispersion instead of a mean and pooled standard deviation (Akinshin).\n\\[\nd = {{\\bar{Y} - \\bar{X} } \\over {s}} \\approx {{Q_{0.5}(Y) - Q_{0.5}(X)} \\over {PMAD_{XY}}} = \\gamma_{0.5}\n\\] This allows us to have an effect size measure that is resilient to feature shape and outliers. The effect size measures are compared to a user defined substantivity cutoff (function utilizes a 0.1 default, but recommends users to adjust for use case). If the effect size change for a feature exceeds the cutoff, the difference is labeled as substantive. Otherwise, differences are labeled as insubstantial.\nThe final stage’s optionality is defined by the user, and based on the outcomes of the Kolmogorov–Smirnov test and effect size measure. During parameterization, the user is able to specify a list of “significance and substantiveness codes” for features that they want the final stage of analysis to be performed on. The codes are pairs of plus or minus signs divisioned by a forward slash (+/+, +/-, -/-). The first element in the pair denotes the result of the Kolmogorov–Smirnov test (+ for pass, - for fail), and the second for if the change detected was substantive (+ for substantive, - for insubstantial). The user can select multiple or all codes to analyze multiple/all categories of features. To skip the final stage altogether, the user can pass an empty list to the function and no features will go through the final stage.\nNote that the combination “-/+” is impossible as when a feature fails the Kolmogorov–Smirnov test, the effect size measure is automatically set to zero.\nThe final stage of analysis creates confidence intervals for the differences in mean, standard deviation, median, interquartile range, kurtosis and skewness of a feature based on bootstrapped sampling distributions from both the training and benchmark datasets. Bootstrapping is a very powerful and robust statistical method due to its nonparametric nature; however, it is extremely computationally expensive and time consuming. Testing estimated that bootstrapping for a single statistic while using native, spark-based distributed sampling and aggregation functions, would take over twenty-four hours for all features in a training and benchmark dataset (both around two million observations). Therefore, alternative computation strategies were tested. Parallelization of computation resulted in the best results, allowing for all statistics to be bootstrapped for features of significant feature drift, within one hour of run time.\nThis efficiency of computation was achieved through parallelizing a single spark-context to all workers within a spark cluster. Parallelizing a spark-context does require flat, non-distributed data, meaning that this methodology only works on reasonably large datasets. To save available memory on workers and increase computation speed, features are flattened and parallelized individually and sequentially. This allows features not in current use to be deallocated from memory. The function also leverages nested structs to compute and store multiple statistics from a single random sample into a distributed dataframe. Once all bootstrapped statistics are calculated, the nested structs are unnested and the bootstrapped sampling distributions (empirical samples) are created. Confidence intervals for each statistic are then derived from the differences of these distributions.\nThe cluster I tested the functions on had 26 workers, each with 16 cores and 128 gb of memory. The parameters for parallelizing bootstrapped samples should be adjusted via the function's parameters for different cluster configurations.\nAfter analysis is finished, the function outputs a table of shape and center metrics of all numeric features tested, CDF and PDF graphs showing differences in features’ distribution, visualized empirical samples derived from bootstrapping, and a table of statistics and confidence intervals from empirical samples.\n\n\ntestProportions\nThe testProportions function identifies drift in all categorical variables shared by the training and benchmark data sets. The function calculates an absolute proportional difference for the categories in each categorical variable, which is then compared to a user-defined ACPD (absolute categorical proportional difference) cutoff. If the value calculated for the feature exceeds the user defined cutoff, the feature is flagged and graphics of the proportional difference are created.\nThe absolute proportional difference is calculated as…\n\\[\n\\sum_{i = 1}^{n}|Proportion\\;of\\;Category_{\\:i,\\:train} - Proportion\\;of\\;Category_{\\:i,\\:test}|\n\\]\nwhere n is the total number of categories in a feature. It should be noted that this measure has a tendency to bloat when n is large. Thus, average ACPD, n, and largest categorical difference accompany ACPD in the function’s output. Comparing all metrics provides data scientists the opportunity to better understand nuances within categorical drift than if only ACPD was provided.\nThe function is computationally robust as it derives the proportions of categories via counts calculated by Pyspark native distributed functions.\nAfter analysis is finished, the function outputs a table of features’ category counts, proportions, and differences, graphs of all numeric features’ categories with the largest proportional differences, and a summary table of all features’ proportional drift statistics.\n\n\ntestNull\nThe testNull function compares differences in features’ null rates. The function does this by calculating and comparing the proportions of null rates in features as shown below.\n\\[\n{Proportion \\; of \\; Null \\; in \\; F_{\\;train} - Proportion \\; of \\; Null \\; in \\; F_{\\;bench}}\n\\over\n{Proportion \\; of \\; Null \\; in \\; F_{\\; bench} + 0.1}\n\\]\nwhere F is the feature being tested.\nThis statistic was designed to be more sensitive towards null rates that have a history of being close to zero. Consider that a feature with a benchmark proportional null rate of 0 and a training proportional null rate of 0.05 returns a value of 0.5, while a feature with a benchmark proportional null rate of 0.9 and a training proportional null rate of 0.95 returns a value of 0.056. This discrepancy in sensitivity was included into the function to direct the attention of users towards features that break historic trends - particularly trends that went from low rates of nulls to high rates of nulls. Features that produce a statistic above a user defined cutoff will be flagged and visualized graphically. There is no default cutoff; however, typically values of 0.1 or above are significant.\nThe function is computationally robust as it derives the proportions of categories via counts calculated by Pyspark native distributed functions.\nThe function outputs a summary table of null rate difference statistics for each feature tested, and bar charts of features with a substantial null rate difference.\n\n\ntestMultivariateNumericOutlier\nThe testMultivariateNumericOutlier function performs nonparametric outlier analysis to give an estimated number of anomalous data points within a dataset, based solely on the values of numeric features. To identify anomalous data points, the function trains and utilizes an isolation forest algorithm. Isolation forests are a tree based ensemble machine learning algorithm (much like a random forest) that provide a nonparametric approach to detecting outliers. The main difference between isolation forests and other ensemble tree algorithms, is that instead of formulating predictions, it generates anomaly scores for each data point. This score, s(xi , N), for an individual point can be calculated as\n\\[\ns(x_i, N) = 2^{{-E(h(x_i))} \\over {c(N)}}\n\\]\nwhere E(h(xi )) is the average path length for the instance (data point) i across all trees in the forest, and c(N) is the average depth in an unsuccessful search in a binary search tree (S. Hariri).\n\\[\nc(N) = 2H(n-1) - ({{2(n-1)} \\over {n}})\n\\]\nwhere n is the number of data points used in tree construction and H(x) is the harmonic number estimated by…\n\\[\nH(x) = ln(x) + \\gamma,\n\\]\nwhere \\(\\gamma\\) is Euler’s constant which can be substituted for an estimate of 0.5772156649. The anomaly score produced ranges from zero to one, with values closer to one being more anomalous. The isolation forest model uses an user defined value cut off, of which 0.5 was selected for this function.\ni.e. any data point with a score greater than or equal ot 0.5 was labeled as an anomaly\nSpark doesn’t have a native function for creating isolation forests. Therefore, the function utilizes Scikit-Learn to train and create the forest. This means that the forest has to be trained on a single node and be trained with flattened data. However, the function does distribute the “prediction” or assignment of anomaly scores. The function achieves distribution through a custom SQL user defined function that broadcasts a sklearn scaler and fit classifier. When the user defined function is called on a distributed data frame to create the scores, the scalar and classifier objects are copied and passed to each worker node within a cluster. This gives each worker node the capability to encode and calculate the anomaly score for any given data point. Since the calculations of scores are independent, each node is then able to calculate the scores for the data points within their own partition of the distributed data frame simultaneously to all the other nodes. For very large datasets, this distribution of calculation significantly shortens overall function run time. After anomaly scores are calculated, potential anomalous data points are counted by the function. The count of regular and anomalous data points are then outputted as a table for the user.\nOptionally, the function is also able to visualize multivariate outliers graphically. By performing principal component analysis on the features used to train the isolation forest, the function is able to calculate the first two principal components for every data point. Using these principal components as coordinates, the function graphs the points in a two-dimensional scatter plot where each data point is colored by its anomalous classification (normal/outlier). Graphing the data points allows data scientists to potentially identify clusters of anomalous data points. However, it should be noted that one should always consider the accounted variance of the principal components generated before accepting or rejecting the validity of the visualization. The data this function was tested on tended to perform well with principal component analysis, allowing for the first two principal components to regularly account for at least ninety-eight percent of explained variance.\n\n\nSummarizizng by Group Functions (Grouped EDA)\nThe two following functions are simple functions that can be used to summarize features in a Pyspark dataframe via a grouping variable. These functions are not necessarily for feature drift detection, but rather provide a quick an easy way to produce descriptive statistics and aggregates of features grouped by a specified variable.\nBoth functions achieve computational robustness by utilizing native Pyspark functions to calculate the grouped statistics and aggregates\n\nsummarizeCategoricalByGroup\nThis function produces a table of counts and proportions of every category in a categorical feature by the grouping variable. If specified in parameterization, the function will also produce barcharts illustrating the proportion of the grouping levels in each category within a feature.\n\n\nsummarizeNumericByGroup\nThis function produces a summary table of statistics for inputted numeric features, aggreated by a grouping variable. If specified in parameterization, the function will also produce boxplots illustrating the numeric distributions of the grouping levels within a numeric feature.\n\n\n\nDatabrick Based Functions\nThese following function is useful for generating reports if one is working within Databricks Notebooks and filesystem.\n\nlogNotebook\nThe logNotebook function allows a user to programmatically save Databrick notebook runs as rendered html documents to Databrick’s file system. The function can also either archive a link to the rendered document via MLflow or return the link as a Python string for alternative storage method.\nThis function is extremely useful for sharing and distributing the results of notebook runs to anyone with access to your organization’s Databricks instance. It allows data scientists, analysts, or engineers who work within Databricks to quickly share reports and figures to individuals within or outside of their teams. All a user has to do is run the function, and then share the generated link. Additionally, since the generation is programmatic, it can be utilized to archive the results of scheduled notebook runs.\nThe sharing of report links is also secure, as system administrators are able to restrict and control user access to Databrick’s file system. Thus, if a link is leaked, individuals outside of the organization, or non-vetted users, are not able to download reports via the leaked generated link."
  },
  {
    "objectID": "portfolio/index.html",
    "href": "portfolio/index.html",
    "title": "Portfolio",
    "section": "",
    "text": "Reconstruction of 19th Century Data Visualization\n\n\n\n\n\n\nR\n\n\nggplot\n\n\nData Visualization\n\n\n\nA reconstruction and modernization of a 1801 William Playfair Graph.\n\n\n\n\n\nOct 10, 2024\n\n\nDane Winterboer\n\n\n\n\n\n\n\n\n\n\n\n\nPyspark (Databricks) Library for Feature Drift Detection\n\n\n\n\n\n\nPython\n\n\nPyspark\n\n\nMachine Learning\n\n\nFeature Drift Detection\n\n\n\nA library of Pyspark functions that performs feature drift detection, grouped EDA, and other useful tasks in Databricks.\n\n\n\n\n\nAug 10, 2024\n\n\nDane Winterboer\n\n\n\n\n\n\n\n\n\n\n\n\nMizzou Datafest 2024: Best-in-Show Submission\n\n\n\n\n\n\nR\n\n\nCompetition\n\n\nData Visualization\n\n\nMachine Learning\n\n\nModeling\n\n\nExploratory Analysis\n\n\n\nAn Analysis of Student Success Indicators in CourseKata Online Textbooks\n\n\n\n\n\nApr 10, 2024\n\n\nDane Winterboer\n\n\n\n\n\n\nNo matching items"
  }
]